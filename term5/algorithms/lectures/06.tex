\date{October 13, 2016}
\author{Egor Suvorov}

\section{Перцептрон}

Продолжение темы <<линейное программирование>>.
Пусть у нас есть вот такой вот граф с весами на входных рёбрах:

in1 ->-> out
in2 -^
in2 -^

На вход поступают нули и единицы, а выход регулируется правилом:
\[
	\left( \sum w_i in_i \ge 1 \right) \iff out = 1 else out = -1
\]

Задача обучения.
Пусть у нас есть набор пар $(in_k, y_k)$, где $in_k$ "--- вектора на вход,
а $y_k$ "--- требуемый выход на соответствующем $in_k$.
Хотим найти такие веса $\vec w$ что:
\[
	\forall k \colon (\langle w, in_k \rangle \ge 1) = y_k
\]

\begin{Rem}
	Можно смотреть на это дело как на задачу разделения $n$-мерных векторов гиперплоскостью,
	потому что $\langle \rangle \ge 1$ можно перевести в $\langle \rangle > 0$.
\end{Rem}
\begin{Rem}
	А можно домножить вектора с $y_k=-1$ на минус единицу и потребовать,
	что все скалярные произведения были положительны.
\end{Rem}

Это задача линейного программирования с неизвестными $w_i$.
Способы решения:
\begin{enumerate}
	\item
		Это задача линейного программирования.
		Натравим на неё симплекс-метод, метод эллипсоидов или метод внутренней точки.
		Симплекс-метод мы знаем, скоро узнаем про метод эллипсоидов.
	\item
		Градиентный спуск.
		Для этого надо минимизировать что-то дифференцируемое.
		Давайте скажем, что у нас ошибки есть отдельно для каждого $k$, а потом просто сложим.
		Для каждого $k$ мы хотим некоторую функцию от $\vec w$, которая сначала маленькая,
		а если ответ пересекает границу неправильного, то она резко возрастает.
		Подойдёт экспонента:
		\[
			e^{-y_k \cdot (\langle w, in \rangle - 1)}
		\]
		Получили функцию, которую можно минимизировать градиентным спуском.
		Внезапно оказывается, что на реальных данных это отлично работает.
	\item
		Будем смотреть на задачу <<разделить вектора гиперплоскостью>>.
		Алгоритм: положим $w=0$, проверим ответ.
		Если для некоторого $x_k$ ответ неверный, то прибавим или вычтем к вектору $w$ вектор $x_k$,
		тем самым уменьшив или увеличив $\langle w, x_k \rangle$.
		Это наверняка поправит ответ.
\end{enumerate}

\begin{theorem}
	Алгоритм 3 сходится за конечное число шагов, если есть ответ.
\end{theorem}
\begin{proof}
	Пусть есть некоторый ответ $\vec u$, давайте отнормируем его $|\vec u|=1$.
	Обозначим $\alpha = \min_k \langle \vec u, \vec x_k \rangle$.
	Это что-то неотрицательное, так как $u$ "--- ответ.

	Оценим длину $w_k$ ($w_k$ "--- значение весов после $k$-го шага).
	Исходно $w_k \ge 0$.
	А после $k$ шагов:
	\[ |w_k| \ge \langle w_k, u \rangle = \sum \langle<x_{ik}, u>\rangle \ge \alpha \cdot k \]

	Теперь оценим по-другому:
	\[ \langle w_{k+1}, w_{k+1} \rangle = (w_k + x_i)^2 = w_k^2 + \underbrace{2w_kx_i}_{<0\text{~по выбору x_i на шаге}} +x_i^2 \le w_k^2 + x_i^2 \]
	Значит, на каждом шаге $w_k^2$ растёт не более, чем на $x_i^2$.
	Обозначим $\beta \coloneq \max x_j^2$.
	Тогда:
	\[ w_{k+1}^2 \le \beta(k+1) \]
	\[ |w_{k+1}| \le \sqrt{\beta}\cdot\sqrt{k+1} \]

	Итого получили оценку с двух сторон на $w_k$:
	\[ \alpha k \le w_k \le \sqrt{\beta}\cdot \sqrt k \]
	Значит, $k$ ограничено (т.к. левая часть растёт асимптотически быстрее).

	Более того: если отнормировать все изначальные вектора ($|x_j|=1$),
	то $\beta=1$, получаем оценку для $k$:
	\[ l \le \left(\frac{1}{\alpha}\right)^2 \]
\end{proof}

Давайте посмотрим, когда метод сходится хорошо, а когда плохо.
Сходится хорошо, когда у нас вектора сильно отличаются от плоскости-ответа.
\TODO картинка
Сходится плохо, когда у нас вектора идут близко к плоскости-ответу.

Другими словами: чем меньше конус, в который можно запихнуть все вектора, тем быстрее сходится метод.
На реальных данных работает, оказывается, хорошо.

\subsection{Сведение LP}
	Пусть была задача линейного программирования:
	\begin{gather*}
		A x_i \le b \\
		x \ge 0 \\
		\langle c , x \rangle \to \max
	\end{gather*}
	Идея: от последнего ограничения избавимся бинпоиском по ответу, а от $b$ избавимся, добавив лишнюю размерность.

	\TODO точка XX.

	Теперь формально: мы хотим по Тьюрингу свести задачу LP к задаче поиска решения неравенства $A \cdot \vec x > \vec 0$.

	Мы считаем, что все условия вроде $x \ge 0$ мы в матрицу запихивать умеем сразу (это был пункт 2 на лекции).

	Давайте научимся сводить задачу $A x \le b$ к $A x > 0$ (это был пункт 3 на лекции):
	\[
		A x \le b \\
		-Ax + b \ge 0
	\]
	Теперь справа в матрицу $-A$ допишем вектор-столбец $b$, получим почти нужное: $A' x \ge 0$,
	только мы хотим, Чтобы последний элемент $x$ (он новый, появился от припысывания к матрицы,
	назовём его $z$) был строго больше нуля.
	Тогда если мы загоним это в оракула, то он сможет нам найти решение с $z > 0$, но $A' x > 0$
	(вместо $\ge 0$).
	Если нашёл "--- повезло.

	Если не нашёл "--- то если ответ на исходную задачу был, то у нас область, ограниченная условиями, вырожденная.
	(например, многоугольник вместо многогранника).
	На реальных данных фиг такое произойдёт "--- если произошло, то у нас есть какая-то линейная зависимость.
	Если всё же произошло "--- добавим некоторый $\eps$ к $b$, немножко раздвинув область, она
	перестанет быть вырожденной.
	Какой $\eps$ "--- подгоним, потому что более аккуратно с вещественными числами мы работать не умеем.


	На доске было написано следующее в точке XX:
	\begin{enumerate}
		\item
			Сначала найдём какое-нибудь начальное решение симплекс-методом: некоторый $x$, для которого $A x \le b$ и $x \ge 0$.
			Мы получили некоторое решение $m = \langle c, x \rangle$.
			Потом давайте найдём правую границу для бинпоиска по ответу: сначала её увеличиваем в два раза,
			а потом внутри пускаем бинпоиск.
	\end{enumerate}

\section{Метод эллипсоидов}
	Нам когда-то рекламировали, что линейное программирование решается за полином.
	Полином на самом деле $n$, $m$, $l$, где $l$ "--- это число бит в двоичной записи $A$, $b$, $c$
	(считаем, числа с фиксированной запятой).

	Давайте научимся решать задачу $A x \le b$ (без максимизации функции).
	К ней мы уже умеем сводить задачу максимизации.

	Метод эллипсоидов: сначала выбрали большой-большой эллипсоид (такой шарик),
	в который всё пересечение полупространств, задаваемое $A x \le b$, точно помещается.
	Оказывается, что можно выбрать шарик радиусом $(2 ^ l)^n$ (тут $2^l$ "--- максимальная координата,
	а возведение в $n$ "--- это мы записали по теореме Крамера координаты пересечений полуплоскостей;
	без более строгого доказательства).
	На практике мы уже заранее не хотим получить большие числа, поэтому пишем $R=10^9$
	или какое-нибудь другое разумное ограничение из физических соображений.

	\begin{Def}
		Эллипс с центром в нуле:
		\[
			\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1
		\]
		$n$-мерный эллипс с центром в нуле:
		\[
			\frac{x_1^2}{a_1^2} + \frac{x_2^2}{a_2^2} + \dots + \frac{x_n^2}{a_n^2} = 1
		\]

		Эллипсоид с центром в нуле: пусть есть положительноопределённая матрица $D$ и вектор-столбец $x$:
		\[
			\langle x^T, D^{-1} x \rangle = 1
		\]
		Это обобщение $n$-мерного эллипса, который можно растягивать в любых направлениях.
		Например, для эллипса у нас диагональная матрица с $r^2$ на диагонали.
	\end{Def}

	Дальше цель такая: берём исходный эллипсоид.
	На каждом шаге, если его центр лежит внутри многогранника, то мы выиграли.
	Иначе многогранник целиком лежит по одно полуплоскость от центра.
	Давайте найдём эллипсоид меньшей площади, который целиком покроет этот кусок старого эллипсоида.
	Если на каждом шаге объём уменьшается хотя бы в $\frac{n+1}{n}$ раз, то центр когда-нибудь попадёт внутрь многогранника.

	Как искать эллипсоид?
	Давайте сначала найдём вектор $a_k$, который показывает из центра старого ($z_k$) в ту сторону, где лежит многогранник.
	Тогда новый эллипсоид ищется по формулам:
	\begin{gather*}
		w_k \coloneq D_k \cdot a_k \\
		r_k \coloneq \sqrt{a_k^{\top} D_k a_k} = \sqrt{a_k^{\top} w_k} \\
		z_{k+1} = z_k + \frac{1}{n+1} \frac{w_k}{|r_k|}
		D_{k+1} = \frac{n^2}{n^2 - 1}\left(D_k - \frac{2}{n+1}\cdot \frac{\overbrace{w_k \cdot w_k^{\top}}^{\text{матрица умножения координат вектора}}}{|r_k|^2} \right) =
			\frac{n^2}{n^2-1}\left(D_k-\frac{2}{n+1}\cdot\frac{(D_ka_k)(a_k^\top D_k^\top)}{a_k^\top D_k a_k}\right)
	\end{gather*}
	Пока не доказываем.
	Но заметим, что один шаг работает за $\O(n^2)$, потому что у нас везде умножаются матрица на вектор.

	Учимся что-то понимать про эллипсоиды: пусть есть эллипсоид с центром в нуле и матрицей $D$.
	Давайте подставим в его уравнение единичный вектор $\vec a$:
	\[ a^{\top} D^{-1} a = x \neq 1 \]
	Теперь мы знаем, на сколько надо домножить вектор $a$, чтобы прийти в точности в границу:
	\[
		r = \frac{1}{\sqrt x} = \frac{1}{\sqrt{a^{\top} D^{-1} a}}
	\]

	Давайте проверим, что после у нас по направлению $a_k$ произошло сжатие эллипсоида после перехода $D_k \to D_{k+1}$.
	Подставим, получим:
	\[
		a_k^\top D_{k+1} a_k = \frac{n^2}{n^2-1}(a_k^\top D_k a_k - \frac{2}{n+1} a_k^\top D_k a_k) =
		\frac{n^2}{n^2-1} \cdot \frac{n-1}{n+1} \cdot a_k^\top D_k a_k =
		\frac{n^2}{(n+1)^2} a_k^\top D_k a_k =
	\]
	Вот во столько раз изменился квадрат расстояния до границы эллипсоида по направлению $a_k$.

	А если подставить направление, перпендикулярное $a_k$, то у нас объем уменьшится в $\sqrt{\frac{n^2}{n^2-1}}$.
	\begin{lemma}
		Тогда объём эллипсоида домножился не более чем на $\frac{n^2}{n^2+1}$.
		И даже не более чем на $\frac{2n}{2n+1}$.
	\end{lemma}
	\begin{proof}
		Без доказательстсва.
	\end{proof}

	Теперь мы можем оценить количество шагов в методе эллипсоидов: был большой эллипсоид объёма $V_0$,
	а внутри была маленькая штука объёма $V_{end}>0$, то за столько шагов сойдётся:
	\[
		\log \frac{V_n}{V_{end}} / \log \frac{2n+1}{2n} = \Theta(n \cdot \log{V_0}{V_end})
	\]
	А время надо домножить ещё на $n^2$.
	Исходный объём огроменный, поэтому получим время работы $\O(n^3 \cdot \log) = \O(n^6 \cdot l)$.
	На практике же у нас этот логарифм отношения объёмов маленький.
