\begin{exmp}
    $\xi_n \in \Z$, $P(\xi_n = k) = q_k$.
    Тогда матрица перехода (бесконечная) этой цепи будет выглядеть вот так:

$\begin{matrix}
\ddots & \vdots & \vdots & \vdots & \reflectbox{$\ddots$}\\
\dots  & q_{-1} & q_0    & q_1    & \dots \\
\dots  & q_{-1} & q_0    & q_1    & \dots \\
\dots  & q_{-1} & q_0    & q_1    & \dots \\
\reflectbox{$\ddots$} & \vdots & \vdots & \vdots & \ddots
\end{matrix}$

Этот процесс вообще не зависит от последнего состояния, поэтому все строки одинаковы
\end{exmp}

\begin{exmp}
А теперь рассмотрим сумму случайных величин: $S_n = \sum\limits_{k=0}^n \xi_k$. 
Этот процесс называется случайным блужданием: у нас есть точка в пространстве, каждую секунду мы ее двигаем на какой-то случайный вектор $\xi_n$.

Ему уже будет соответствовать такая матрица:

$\begin{matrix}
\ddots & \vdots & \vdots & \vdots & \vdots & \reflectbox{$\ddots$}\\
\dots  & q_{-1} & q_0    & q_1    & q_2    & \dots \\
\dots  & q_{-2} & q_{-1} & q_0    & q_1    & \dots \\
\dots  & q_{-3} & q_{-2} & q_{-1} & q_0    & \dots \\
\reflectbox{$\ddots$} & \vdots & \vdots & \vdots & \vdots & \ddots
\end{matrix}$

Каждый раз строка сдвигается на 1 вправо, так, чтобы на диагонали стояли вероятности остаться на месте: $P_{ii} = q_0$.
\end{exmp}

\begin{exmp}
Очередь: $q_k$ -- вероятность того, что за единицу времени подойдет $k \geq 0$ человек. 
Каждую единицу времени, если очередь не пуста, то ее длина уменьшается на $1$.

Состояние процесса $\xi_n$ -- длина очереди в момент времени $n$.

Матрица перехода: (здесь она уже бесконечна вправо и вниз, но имеет левый верхний угол)

$\begin{matrix}
q_0    & q_1    & q_2    & \dots \\
q_0    & q_1    & q_2    & \dots \\
0      & q_0    & q_1    & q_2 & \dots \\
0      & 0      & q_0    & q_1 & \dots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{matrix}$

Если очередь была пуста, то мы переходим в состояние $i$ с вероятностью $q_i$.
Если же очередь не пуста, то ее длина точно уменьшается на $1$, а потом сколько-то человек приходит.
Поэтому, переходим в состояние $i-1$ с вероятностью $q_i$.
\end{exmp}

\begin{exmp}
$\xi_n$ -- длина серии успехов в схеме Бернулли.

$\begin{matrix}
q & p & 0 & 0 & 0 & \dots  \\
q & 0 & p & 0 & 0 & \dots \\
q & 0 & 0 & p & 0 & \dots \\
q & 0 & 0 & 0 & p & \dots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{matrix}$
\end{exmp}

\begin{exmp}
Все примеры были либо с бесконечной, либо с полубесконечной матрицей. 
А теперь один из самых известных примеров с конечной матрицей -- модель Эренфестов.

Есть камера, разделенная мембраной на две одинаковые половины, в ней $N$ частиц.
Один такт -- переход частицы слева направо или справа налево. Для каждой частицы считаем, что у нее одинаковые шансы совершить переход.

Состояние $\xi_n$ -- количество частиц слева от мембраны, $\xi_n \in \{0, 1, \dots, N\}$.

$\begin{matrix}
0            & 1            & 0                & 0               & \dots \\
\frac{1}{N}  & 0            & \frac{N - 1}{N}  & 0               & \dots \\
0            & \frac{2}{N}  & 0                & \frac{N - 2}{N} & \dots \\
\vdots       & \vdots       & \vdots           & \vdots          & \ddots\\
\dots & \dots & \dots & \dots & \dots &\frac{N}{N} & 0
\end{matrix}$
\end{exmp}

\begin{exmp}
Ветвящиеся процессы (например, численность популяции).

Есть популяция особей, которые не мешают друг другу и нет проблемы перенаселенности, поэтому численность зависит только от скорости размножения.
Единица времени = поколение (за которое старые умерли, но успели родить новое поколение).

$\nu$ -- количество потомков у особи, $q_k = p(\nu = k), k = 0, 1, 2, \dots$;
$\xi_n$ -- численность популяции.

$P_{ij} = P(\nu_1 + \nu_2 + \dots + \nu_i = j)$. Несложно понять, что это коэффициенты при $t^j$ в $\phi^i(t)$, где $\phi$ -- производящая функция для $\nu$

Интересен вопрос о вырождении популяции, когда $\xi_n = 0$ (и в дальнейшем, понятное дело, популяция так и останется нулевой).

Рассмотрим $z_n = P(\xi_n = 0)$. Эта величина возрастает с ростом $n$ (так как из нуля мы не уходим, но можем туда прийти из другого состояния).
Также, $z_n$ ограничена единицей, следовательно, имеет предел $z$, назовем его вероятностью вырождения популяции.

Обозначим за $\phi_n(t)$ -- производящую функцию прироста за $n$ шагов ($\phi = \phi_1$ это, соответственно, производящая функция для $\nu$).
Запишем формулу полной вероятности: 
\begin{gather*}
\phi_{n+1}(t) = \phi(\phi_n(t)) = \underbrace{\phi \circ \phi \circ \dots \circ \phi}_{n} \\
z_n = \phi_n(0) \\
z_{n+1} = \phi(z_n), n \to \infty \Ra z = \phi(z)
\end{gather*}

Оказывается, что верно следующее: если $E \nu \leq 1$, то $z = 1$. Иначе существует корень (единственный) уравнения $z = \phi(z)$, принадлежащий интервалу $(0, 1)$.
И именно этот корень и является вероятностью вырождения популяции.
\end{exmp}

\section{Классификация состояний марковской цепи}
\begin{Def}
    Пишут $i \to j$ ($j$ достижимо из $i$), если существует такое $m$, что $P_{ij}(m) > 0$. 

    Если $i \leftrightarrow j$, то эти состояния сообщающиеся (из $i$ достижимо $j$ и наоборот).
\end{Def}

\begin{Def}
$i$ существенно, если $\forall j \colon i \to j \Ra j \to i$. На пальцах: нельзя из состояния уйти так, чтобы потом было не вернуться обратно.

Если это не так, ($\exists j \colon i \to j, j \not\to i$), то состояние несущественно.
\end{Def}

Все существенные состояния делятся на классы эквивалентности ($\leftrightarrow$ -- отношение эквивалетности). 
Эти классы называются эргодическими и обычно нумеруются так: ЭК1, ЭК2, и так далее.

И еще выделяют класс несущественных состояний.

\begin{Def}
Цепь называется неприводимой, если все ее состояния в ЭК1.
\end{Def}

\begin{Def}
Пусть $M_j = \{n \in \N \colon P_{ij}(n) > 0 \}$ -- множество количеств шагов, за которые можно вернуться в состояние $j$.
$M_j$ замкнуто относительно сложения: если можно вернуться за $i$ шагов и за $j$ шагов, то можно и за $i + j$.

У этого набора чисел есть период $d_j = gcd(M_j)$. 
\end{Def}

\begin{exmp}
Период блуждания коня по шахматной доске равен 2 (так как граф двудольный).
\end{exmp}

\begin{theorem}
Начиная с некоторого $k$ все $k d_j \in M_j$
\end{theorem}
\begin{Rem}
Теорема верна для любого аддитивного класса.
\end{Rem}
\begin{proof}
Пусть $n_1, \dots, n_m \in M$ и $gcd(n_1, \dots, n_m) = d$.

Тогда существуют $a_1, \dots, a_m \in \Z$ такие, что $d = \sum a_i n_i$.

Это равенство можно переписать: $$d + \underbrace{\sum\limits_{a_i < 0} -a_i n_i}_{b} = \sum\limits_{a_j > 0} a_j n_j$$ 
Обе суммы лежат в $M$ как суммы элементов $M$ с положительными коэффициентами. То есть, $b, b + d \in M$. 
При этом, $b$ делится на $d$ (так как все $a_i$ на него делятся)

Возьмем произвольное большое число $N > b^2, N \vdots d$, покажем, что $N \in M$.

Поделим $N$ на $b$ с остатком: $$N = kb + r, r < b; b \vdots d, N \vdots d \Ra r \vdots b$$

Откуда, $$N = kb + ld = (k - l) b + l (b + d)$$ 
Знаем, что $$b, b + d \in M, k - l, l > 0$$ 
Тогда эта комбинация тоже лежит в $M$, что и требовалось.
\end{proof}

\begin{assertion}
$i \leftrightarrow j \Ra d_i = d_j$
\end{assertion}
\begin{proof}
$P_{ij}(m) > 0, P_{ji}(l) > 0$.

Для $n = d_jk$ верно: $$P_{ii}(m + l + n) \geq P_{ij}(m) + P_{jj}(n) + P_{ji}(l) > 0$$

Возьмем $k$ достаточно большое, из теоремы.
Тогда, $m + l + k d_j \vdots d_i$, можно увеличить $k$ на единицу и делимость останется. 
Откуда, $d_j \vdots d_i$. И наоборот. Значит, $d_i = d_j$.
\end{proof}
\begin{Rem}
Вроде можно доказывать и без использования теоремы, но на лекции сходу не вспомнили, как.
\end{Rem}

Если есть эргодический класс с периодом $d > 1$, то его можно разбить на подклассы $C_0, C_1, \dots, C_{d - 1}$, где $C_k = \{j \colon \exists m \geq 0 \colon P_{i_0j}(md + k) > 0\}$
для какого-то зафиксированного состояния $i_0$ из этого класса.
По сути, разбили класс на $d$ долей с переходами только из $i$-й доли в $(i + 1) \mod d$-ю.

\begin{Def}

Вероятность первого возвращения за $n$ шагов $f_{jj}(n) = P(\xi_n = j), \xi_{n-1} \neq j, \dots, \xi_1 \neq j \mid \xi_0 = j)$.

При этом, полагаем $f_{jj}(0) = 0$.

Вероятностью возвращения назовем сумму $\sum\limits_{n=1}^\infty f_{jj}(n)$.

\end{Def}

\begin{Def}
Существенное состояние $j$ называется возвратным, если $f_{jj} = 1$.
\end{Def}

\begin{Def}
Для возвратных состояний можно определить среднее время возвращения $\mu_j = \sum n f_{jj}(n)$. 

Также вводят обратную величину $\pi_j = \frac{1}{\mu_j}$. 

Если для возвратного состояния $j$ $\pi_j > 0$, то $j$ называют положительным состоянием. 
Иначе, если $\pi_j = 0$, то состояние нулевое.
\end{Def}

\begin{theorem}
$j$ возвратное $\Leftrightarrow \sum\limits_1^\infty P_{jj}(n) = \infty$
\end{theorem}
\begin{proof}
$$P_{jj}(n) = \sum\limits_{k=1}^n f_{jj}(k) P_{jj}(n - k)$$
Умножим это на $t^n$ и просуммируем по $n$, получим
$$P(t) = \sum\limits_{n=0}^\infty P_{jj}(n) t^n = P(t) F(t) + 1$$
Здесь $P, F$ -- производящие функции для $P_{jj}$ и $f_{jj}$
$$P(t) = \frac{1}{1 - F(t)}$$

Если устремить $t \to 1-0$, то $\sum f_{jj}t^n \to 1$, если $j$ возвратное.
И наоборот, если стремится к 1, то $j$ возвратное.
\end{proof}
