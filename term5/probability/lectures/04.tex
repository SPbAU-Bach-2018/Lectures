\begin{theorem}[Теорема об обратной функции]

    Пусть какая-то функция $F$ удовлетворяет свойствам функции распределения, тогда $F$ -- функция распределения некоторой случайной величины $\xi$.
\end{theorem}
\begin{proof}
    Пусть $F^*(y) = \sup \{x\colon F(x) \leq y\}$.
    Несложно заметить, что $F^* = F^{-1}$, где обратная функция существует. 
    Еще есть точки разрыва и участки, где $F$ константа, там мы $F^*$ не определили, давайте сделаем это.

    Пусть $x$ -- точка разрыва, $F(x-0) = a, F(x+0) = b$, тогда скажем, что $F^*(y) = x, y \in [a; b]$.
    Если $F(x) = y$ на отрезке $[a; b]$, то $F^*(y) = a$.

    Отлично, доопределили обратную функцию, $F^*$ называется обобщенной обратной функией.
    Еще верен факт, что $F^{*^*}(x) = F(x)$, разрывы переводит в участки постоянства и наоборот.

    Возьмем  случайную величину $\eta \sim U[0, 1]$, хотим доказать, что $\xi = F^*(\eta)$.

    $F_{\xi}(z) = P(F^*(\eta) < z) = P(\eta < F(z)) = F_{\eta}(F(z)) = F(z)$ (последний переход так как $\eta$ -- равномерное распределение).
\end{proof}
\begin{Rem}
Важна даже не сколько теорема, сколько ее доказательство. Оно полезно, если хотим сгенерировать какую-то случайную величину, зная ее распределение и имея только
генератор равномерно распределенной случайной величины (например, функция \t{rand()} в каком-нибудь языке программирования).

Например, хотим сгенерировать монетку, с вероятностью орла $p$. Мы знаем ее функцию распределения $F(x) = \begin{cases}p, x \in [0, 0.5] \\ 1, x \in (0.5, 1]\end{cases}$.
Строим $F^*(y) = \begin{cases} 1, y \in [p, 1] \\ 0, y \in [0, p) \end{cases}$

А теперь берем генератор равномерно распределенных случайных чисел с отрезка $[0; 1]$, подставим число, которое оно выдал, в обратную функцию, получаем единичку или нолик с нужными вероятностями.
\end{Rem}


\section{Случайные векторы и их распределения}

\begin{Def}
Вектор $\vec{\xi} = (\xi_1, \dots, \xi_n) \colon \Omega \to \R^n$ называется случайным вектором, если все его координаты $\xi_k$ -- случайные величины.
\end{Def}                                                                                                                                        
\begin{Rem}
Значок вектора будет часто опускаться, если из контекста понятно.
\end{Rem}

Борелевские множества определаются аналогично одномерному случаю: определяем лучи как $(-\infty, \vec{x}) = (-\infty, x_1) \times \dots \times (-\infty, x_n)$, а потом берем их $\sigma$-замыкание.

Аналогично определяется и функция распределения: $F_{\vec{\xi}}(\vec{x}) = P_{\vec{\xi}}((-\infty, \vec{x}))$.                                                                                                                                       

\begin{theorem}[Свойства функции распределения]

Пусть $F$ -- функция распределения. Тогда:

\begin{enumerate}
\item $F(x) \nearrow$ по каждой переменной.

\item $\exists i\colon x_i \to -\infty \Ra F(x) \to 0$, а так же $F(x) \to 1$, если $\forall i\colon x_i \to \infty$.

\item $F$ непрерывна слева по каждой переменной.

\end{enumerate}
\end{theorem}

Если мы живем в двумерном случае и есть квадратик $[a, b] \times [c, d]$, то $P(\xi \in \square)= F(b, d) - F(a, d) - F(b, c) + F(a, c)$. 
Соответственно, в общем случае тут просто формула включений-исключений.

Проблема: в одномерном случае есть теорема об обратной функции, с помощью которой для любой функции с нужными свойствами можно построить соответствующее распределение.
В многомерном случае этих свойств не достаточно. Нарисуем график следующей функции $\R^2 \to \R$.

\begin{tikzpicture}
    \draw[<->] (0, 5) -- (0,0) --  (5, 0);
    \draw ((1, 5) -- (1, 2) -- (2, 1) -- (5, 1);
    \draw[color=red] (1.25, 1.25) rectangle (3, 3);
    \node at (0.5, 0.5) {$0$};
    \node at (2.5, 0.5) {$0$};
    \node at (0.5, 2.5) {$0$};
    \node at (3.5, 3.5) {$1$};
\end{tikzpicture}

Несложно заметить, что график этой функции удовлетворяет всем трем свойствам функции распределения. Пусть этому графику соответствует какая-то случайная величина.
Тогда можно взять красный квадратик на картинке и посчитать вероятность того, что $\xi$ находится в этом квадрате. Мы получим: $P(\xi \in \square) = 1 - 1 - 1 + 0 = -1$. 
Такого быть не может, и значит этот график не задает никакую случайную величину и этих трех свойств недостаточно.

Утверждается, что достаточно добавить еще четвертое свойство: для любого квадрата $[a, b] \times [c, d]$ верно: $F(b, d) - F(a, d) - F(b, c) + F(a, c) \geq 0$ 
(опять же, в случае большей размерности, это свойство обобщится до формулы включений-исключений), и тогда теорема об обратной функции в многомерном случае тоже будет верна.

Аналогично одномерному случаю, $\xi$ -- абсолютно непрерывая случайная величина, если она равна 
$$\int\limits_{-\infty}^x p_{\xi}(t)\d t = \int\limits_{-\infty}^{x_1} \d t_1 \dots \int\limits_{-\infty}^{x^n} p(t_1, \dots, t_n) \d t_n$$

Равномерным распределением называют распределение с плотностью $p_{\xi} = \frac{1}{V(D)} \mathbb{1}_D(x)$, где $V(D)$ -- мера Лебега.
Определяют именно через плотность, так как описывать функцию распределения в многомерном случае сложно из-за кучи случаев.

В отличие от одномерного случая, сингулярных распределений будет гораздо больше. Например, можно взять просто отрезок, вложить его в пространство и получить интеграл, равный нулю.

\subsection{Независимые случайные величины}
Задача: есть распределение, хотим по его проекциям восстановить исходное. К сожалению, это сделать не всегда можно однозначно: вот есть три распределения, проекции на оси одинаковые.

\begin{minipage}[c]{0.3\textwidth}
\begin{tikzpicture}
    \draw[<->] (0, 5) -- (0,0) --  (5, 0);
    \draw[pattern=north west lines, pattern color=black] (1, 1) rectangle(4, 4);
\end{tikzpicture}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\begin{tikzpicture}
    \draw[<->] (0, 5) -- (0,0) --  (5, 0);
    \draw (1, 1) -- (4, 4);
\end{tikzpicture}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\begin{tikzpicture}
    \draw[<->] (0, 5) -- (0,0) --  (5, 0);
    \draw (1, 4) -- (4, 1);
\end{tikzpicture}
\end{minipage}

Утверждается, что если координаты независимы, то это все-таки можно сделать.
\begin{Def}
$\xi_1, \dots, \xi_n$ называют независимыми, если $\forall B_1, \dots, B_n \in \mathbb{B}\colon P(\xi_1 \in B_1, \dots, \xi_n \in B_n) = \prod P(\xi_i \in B_i)$.

Или же (равносильно), если $\forall x_1, \dots, x_n\colon F_{\xi}(x) = \prod F_{\xi_i}(x_i)$.
\end{Def}
\begin{theorem}
Если $\xi$ дискретное, то все $\xi_k$ дискретны и независимость $\{\xi_k\}$ равносильна тому, что $\forall x \in \R^n P(\xi=x) = \prod (\xi_k = x_k$).
\end{theorem}
\begin{proof}
Проекции дискретны потому что различных проекций не более чем количество различных точек, а их счетно. 
Распосильность просто по определению.
\end{proof}
\begin{theorem}
$\xi$ абсолютно непрерывная, тогда $\xi_k$ -- абсолютно непрерывны и их независимость равносильна тому, что $p_{\xi}(x) = \prod p_{\xi_k}(x_k)$ в точках непрерывности $p_{\xi}$.
\end{theorem}
\begin{Rem}
Условие про точки непрерывности важное: можно взять, как угодно поменять значение $p$ в одной точке, это будет все еще плотность, а вот теорема сломается.
\end{Rem}
\begin{proof}
Видимо, в одну сторону просто интеграл, а в другую производная? \TODO
\end{proof}

\section{Условная вероятность}
Нам будет необходимо работать с интегралом Стилтьеса. 

Если интеграл Римана определяется как $\int\limits_a^b f(x)\d x = \lim\limits_{\sup x_{k+1}-x_k \to 0} \sum f(x_k)(x_{k+1} - x_k)$, то

интеграл Стилтьеса: $\int\limits_a^b f(x)\d F(x) = \lim \limits_{\sup x_{k+1}-x_k \to 0} \sum f(x_k) (F(x_{k+1}) - F(x_k))$. В каком-то смысле, чем больше плотность в точке, тем она важнее при подсчете интеграла.

В частности, если $F(x) = x$, то это просто интеграл Римана.
\begin{Rem}
На самом деле, этот интеграл у нас появлялся тогда, когда мы хотели сделать какую-нибудь замену переменной и что-то заносили под дифференциал.
\end{Rem}

Интеграл Стилтьеса обладает всеми свойствами интеграла Римана (линейность и прочие).

\begin{Def}
$$P(a \mid \xi = X) = \frac{P(A \cap \{\xi = x\})}{P(\xi = x)} = \begin{cases} \lim\limits_{\epsilon \to 0}  \frac{P(A \cap |\xi - x| < \epsilon)}{P(|\xi - x| < \epsilon)}, \text{если он существует} \\
0, \text{если } \exists \epsilon \colon P(|\xi - x| < \epsilon) = 0 \end{cases}$$
\end{Def}

\begin{Def}
    $F_{\eta}(y \mid \xi = x) = P(\eta < y \mid \xi = x)$ -- условная функция распределения.    
\end{Def}

Непрерывная формула полной вероятности:
$$P(A) = \sum\limits_{k=-\infty}^{\infty} P(\underbrace{\xi \in [x_k - \epsilon, x_k + \epsilon)}_{H_k}) P(A \mid H_k) \xrightarrow[\epsilon \to 0]{} \int\limits_{-\infty}^{\infty} P(A \mid \xi = x) \d F_{\xi}(x)$$

\begin{exmp}
Пусть есть равномерное распределение $\theta \sim U[0, 1]$ и распределение $\xi$, про которое мы знаем, что $(\xi \mid \theta = p) \sim Bin(n, p)$. 
Хочется как-то найти $\xi$, это можно сделать из формулы выше:
$$P(\xi = k) = \int\limits_0^1 P_{n, p}(k) \d p = \int\limits_0^1 {n \choose k} p^k (1 - p)^{n-k} \d p = \frac{1}{n + 1}$$
\end{exmp}

Закон композиции распределений:

Пусть есть $\xi, \eta$ -- независимые случайные величины, хочется что-нибудь узнать про величину $\xi + \eta$:
$$F_{\xi + \eta}(x) = P(\xi + \eta < x) = \int\limits_{-\infty}^{\infty} P(\xi + \eta < x \mid \xi = t) \d F_{\xi}(x) = \int\limits_{-\infty}^{\infty} F_{\eta}(x - t) \d F_{\xi}(t) = F_{\eta} * F_{\xi}$$
Операция ``*'' называется сверткой. 

Для плотностей:
$$p_{\xi + \eta}(t) = \int\limits_{\R} p_{\eta}(x - t) p_{\xi}(t) \d t = p_{\eta} * p_{\xi}$$