% !TeX root = lectures.tex
\setauthor{Егор Суворов}

\chapter{Введение}
\section{Модель}
Теория кодирования работает над \textit{передачей информации} через \textit{канал}.
Терминология берётся из связи, но каналом совершенно необязательно является
Wi-Fi, провод или что-то такое.
Например, магнитная лента или жёсткий диск тоже являются каналами: на них записали информацию,
через какое-то время прочитали.
Тогда шум включает не только ошибки при записи и чтении, но и то, что случилось с
накопителем за время между записью и чтением.

\begin{Def}
	Модель, в которой мы работаем:

	\centerline{
	\xymatrix{
		\text{Источник} \ar[dd]_{\text{информационные\\символы}} & & \text{Приёмник} \\
		& \text{Шум} \ar[d] & \\
		\text{Кодер} \ar[r]_{A} & \text{Канал} \ar[r]_{B} & \text{Декодер} \ar[uu]_{\text{информационные\\символы}} \ar[uu]
	}
	}
\end{Def}

Вся сложность возникает из-за наличия шума.
Мы считаем, что у него есть какая-то разумная модель,
причём характеристики шума со временем не меняются.
Мы не боремся с теми, кто нарочно нам портит канал и <<играет против нас>>.

Чтобы можно было даже в шуме восстановить данные, кодер должен добавлять в данные
какую-то избыточность, чтобы после применения шума её хватило для декодирования.

\begin{Def}
	Мы будем работать с \textit{блоковыми} кодами: на вход поступил блок данных определённого размера,
	мы её закодировали, пропустили через канал, раскодировали.
	Все блоки независимы.
\end{Def}
\begin{Rem}
	Бывают ещё \textit{потоковые} коды, которые кодируют непрерывный поток данных.
\end{Rem}

\begin{Def}
	На вход в кодер поступает \textit{информационный символ} (элемент какого-то множества информационных символов).
	Кодер выдаёт в канал несколько элементов \textit{алфавита $A$}, обычно предполагается,
	что это элементы конечного поля $GF(q)$ (для удобства работы).
	Также считается, что этих элементов выдаётся ровно $n$ "--- \textit{размер кода}.
	Канал выдаёт в декодер столько же элементов \textit{алфавита $B$}, а декодер выдаёт информационный символ
	(такой же, какой поступил в кодер).
\end{Def}

\begin{exmp}
	$A$ может отличаться от $B$.
	Например, кодер может выдавать элементы $A=GF(2)=\{0, 1\}$,
	а после прохождения через канал можем получить элементы $B=\{0, 1, \varkappa\}$.
	Здесь $\varkappa$ означает \textit{затирание} "--- <<мог быть и 0, и 1, непонятно>>.
\end{exmp}

\begin{Def}
	Если информационный символ представляет собой вектор из $k$ элементов алфавита $A$,
	то \textit{скоростью кода} называется следующая величина от 0 до 1:
	\[
		R \coloneqq \frac{k}{n}
	\]
\end{Def}
\begin{exmp}
	Например, если информационный символ "--- это вектор из $k$ бит
	и код "--- это повтор каждого бита трижды, то $n=3k$ и $R=\frac13$.
\end{exmp}
\begin{Rem}
	Когда будем дальше строить алгоритмы, мы будем фиксировать скорость.
	Например: 15 информационных бит кодируем 20 битами.
\end{Rem}
\begin{Rem}
	В реальных системах можно скорость \textit{адаптировать} под канал:
	шума больше "--- уменьшаем скорость, шума меньше "--- увеличиваем.
	Мы этим пока не занимаемся.
\end{Rem}

\begin{Def}
	У каждого канала есть \textit{пропускная способность $C$} "--- число 0 от 1.
	0 означает канал, по которому вообще никакая информация не передаётся (выдаёт случайный результат),
	а 1 означает канал, в котором вообще нет ошибок.
\end{Def}
\begin{Rem}
	Как именно считается эта пропускная способность, пока опустим.
	Но полностью определяется из канала.
\end{Rem}

\begin{theorem}[прямая теорема Шеннона]
	Если у нас есть канал с пропускной способностью $C$ и произвольное число $0 \le R < C$,
	то для любого $\epsilon > 0$ существует код такой, что
	вероятность ошибки (на один переданный блок) меньше $\epsilon$.
\end{theorem}
\begin{Rem}
	Теорема неконструктивная: конкретные коды не предъявляет, доказывается вероятностно.
\end{Rem}
\begin{Rem}
	Это довольно круто: если мы не пытаемся передавать данные быстрее пропускной способности канала,
	то существует код со сколь угодно малой ошибкой на один блок.
	Но за сколь угодно малую ошибку мы платим безумным ростом размера кода ($n$).
	Практического смысла нет.
\end{Rem}
\begin{theorem}[обратная теорема Шеннона]
	Если есть канал с пропускной способностью $C$ и число $R>C$,
	то любой код со скоростью $R$ имеет ошибку хотя бы $\epsilon >0 $.
\end{theorem}
\begin{Rem}
	То есть если мы пробуем передать данные быстрее $C$, то у нас может получиться,
	но уже не получится сделать сколь угодно малую ошибку.
	Возможно, получится сделать её $10^{-5}$, но не меньше.
\end{Rem}
