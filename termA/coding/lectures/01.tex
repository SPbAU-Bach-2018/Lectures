% !TeX root = lectures.tex
\setauthor{Егор Суворов}

\chapter{Введение}
\section{Модель}
\subsection{Общие определения}
Теория кодирования работает над \textit{передачей информации} через \textit{канал}.
Терминология берётся из связи, но каналом совершенно необязательно является
Wi-Fi, провод или что-то такое.
Например, магнитная лента или жёсткий диск тоже являются каналами: на них записали информацию,
через какое-то время прочитали.
Тогда шум включает не только ошибки при записи и чтении, но и то, что случилось с
накопителем за время между записью и чтением.

Ещё применения: в биологии (что-то кодируется в ДНК с избыточностью),
в социологии (составляем опрос так, чтобы исправлять ошибки вроде <<люди соврали>>),
агрономия (как эффективно протестировать удобрения).

\begin{Def}
	Модель, в которой мы работаем:

	\centerline{
	\xymatrix{
		\text{Источник} \ar[dd]_{\text{информационные\\символы}} & & \text{Приёмник} \\
		& \text{Шум} \ar[d] & \\
		\text{Кодер} \ar[r]_{A} & \text{Канал} \ar[r]_{B} & \text{Декодер} \ar[uu]_{\text{информационные\\символы}} \ar[uu]
	}
	}
\end{Def}

Вся сложность возникает из-за наличия шума.
Мы считаем, что у него есть какая-то разумная модель,
причём характеристики шума со временем не меняются.
Мы не боремся с теми, кто нарочно нам портит канал и <<играет против нас>>.

Чтобы можно было даже в шуме восстановить данные, кодер должен добавлять в данные
какую-то избыточность, чтобы после применения шума её хватило для декодирования.

\begin{Def}
	Мы будем работать с \textit{блоковыми} кодами: на вход поступил блок данных определённого размера,
	мы её закодировали, пропустили через канал, раскодировали.
	Все блоки независимы.
\end{Def}
\begin{Rem}
	Бывают ещё \textit{потоковые} коды, которые кодируют непрерывный поток данных.
\end{Rem}

\begin{Def}
	На вход в кодер поступает \textit{информационный символ} (элемент какого-то множества информационных символов).
	Кодер выдаёт в канал несколько элементов \textit{входного алфавита $A$}, обычно предполагается,
	что это элементы конечного поля $GF(q)$ (для удобства работы).
	Также считается, что этих элементов выдаётся ровно $n$ "--- \textit{размер кода}.
	Канал выдаёт в декодер столько же элементов \textit{выходного алфавита $B$}, а декодер выдаёт информационный символ
	(такой же, какой поступил в кодер).
\end{Def}
\begin{Rem}
	Иногда полезно брать не двоичный входной алфавит, а, скажем, троичный:
	например, для идеального канала и 9 информационных символов.
	Тогда если кодировать в троичной системе, то скорость $1$, а если в двоичной, то меньше.
\end{Rem}
\begin{Rem}
	Плюс могут быть физические особенности канала: не два уровня напряжения, а три.
	Или четыре, как пойдёт.
\end{Rem}
\begin{Rem}
	Мы начинаем с \textit{алгебраического кодирования} "--- это когда кодер/декодер реализованы
	методами алгебры.
	Ещё бывает, например, комбинаторное.
\end{Rem}

\begin{exmp}
	$A$ может отличаться от $B$.
	Например, кодер может выдавать элементы $A=GF(2)=\{0, 1\}$,
	а после прохождения через канал можем получить элементы $B=\{0, 1, \varkappa\}$.
	Здесь $\varkappa$ означает \textit{затирание} "--- <<мог быть и 0, и 1, непонятно>>.
\end{exmp}

\begin{Def}
	Если информационный символ представляет собой вектор из $k$ элементов алфавита $A$,
	то \textit{скоростью кода} (rate) называется следующая величина от 0 до 1:
	\[
		R \coloneqq \frac{k}{n}
	\]
\end{Def}
\begin{exmp}
	Например, если информационный символ "--- это вектор из $k$ бит
	и код "--- это повтор каждого бита трижды, то $n=3k$ и $R=\frac13$.
\end{exmp}
\begin{Rem}
	Когда будем дальше строить алгоритмы, мы будем фиксировать скорость.
	Например: 15 информационных бит кодируем 20 битами.
\end{Rem}
\begin{Rem}
	В реальных системах можно скорость \textit{адаптировать} под канал:
	шума больше "--- уменьшаем скорость, шума меньше "--- увеличиваем.
	Мы этим пока не занимаемся.
\end{Rem}

\begin{Def}
	У каждого канала есть \textit{пропускная способность $C$} "--- число 0 от 1.
	0 означает канал, по которому вообще никакая информация не передаётся (выдаёт случайный результат),
	а 1 означает канал, в котором вообще нет ошибок.
\end{Def}
\begin{Rem}
	Как именно считается эта пропускная способность, пока опустим.
	Но полностью определяется из канала.
\end{Rem}

\subsection{Теоремы Шеннона}
\begin{theorem}[прямая теорема Шеннона]
	Если у нас есть канал с пропускной способностью $C$ и произвольное число $0 \le R < C$,
	то для любого $\epsilon > 0$ существует код такой, что
	вероятность ошибки меньше $\epsilon$.
\end{theorem}
\begin{Rem}
	Теорема неконструктивная: конкретные коды не предъявляет, доказывается вероятностно.
\end{Rem}
\begin{Rem}
	Вероятность ошибки можно считать и на блок, и на символ, это пересчитывается.
\end{Rem}
\begin{Rem}
	Это довольно круто: если мы не пытаемся передавать данные быстрее пропускной способности канала,
	то существует код со сколь угодно малой ошибкой на один блок.
	Но за сколь угодно малую ошибку мы платим безумным ростом размера кода ($n$).
	Например, для $n=15$ может не получиться снизить ошибку (слишком случайный шум),
	а вот для $n=10^{100}$ может получиться.
	Размер большой, алгоритм не предъявлен, практического смысла нет.
\end{Rem}
\begin{Rem}
	Большой размер блок плох по двум причинам: вычислительная сложность и большой объём передаваемых данных (а вдруг столько не надо?).
\end{Rem}
\begin{theorem}[обратная теорема Шеннона]
	Если есть канал с пропускной способностью $C$ и число $R>C$,
	то любой код со скоростью $R$ имеет ошибку хотя бы $\epsilon >0 $.
\end{theorem}
\begin{Rem}
	То есть если мы пробуем передать данные быстрее $C$, то у нас может получиться,
	но уже не получится сделать сколь угодно малую ошибку.
	Возможно, получится сделать её $10^{-5}$, но не меньше.
\end{Rem}

Итого: теорема Шеннона, конечно, говорит, <<решение есть>>, но ничего не говорит
ни про алгоритм кодирования, ни про адекватность размеров блока, ни про специфичные
коды под специфичные вида шума, ни про коды, которые всё-таки могут ошибаться.
Поэтому и придумали кучу разных видов кодов.

\subsection{Примеры каналов}
\begin{Def}
	\textit{Двоичный симметричный канал} (ДСК) "--- это канал с $A = B = GF(2) = \{0, 1\}$,
	характеризующийся \textit{вероятностью ошибки} $p$.
	Он преобразует передаваемые символы независимо друг от друга следующим образом:

	\centerline{
		\xymatrix@=2cm{
			0 \ar[r]^{1-p} \ar[rd]^(.3){p} & 0 \\
			1 \ar[r]_{1-p} \ar[ru]_(.3){p} & 1
		}
	}
\end{Def}
\begin{Rem}
	Это, конечно, математическая модель, в жизни ровно такого не бывает.
	Но это хорошая простая модель, можно на ней тестировать алгоритмы.
\end{Rem}
\begin{Rem}
	\textit{Двоичный} потому что передаёт биты, а \textit{симметричный} потому что
	вероятности ошибки одинаковы для 0 и 1.
\end{Rem}
\begin{Rem}
	Самый плохой случай ($R=0$) получаем при $p=\frac12$, тогда канал просто выдаёт случайный шум.
	А вот $p=0$ и $p=1$ одинаково хороши.
\end{Rem}
\begin{Rem}
	Строгая формула для $R$ тоже есть, но мы пока не вводим.
\end{Rem}

\begin{Def}
	\textit{Двоичный стирающий канал} "--- это канал с характеристиками $p_\text{ошибки}$ и $p_\text{стирания}$:
	\begin{gather*}
		\begin{aligned}
			A &= GF(2) = \{0, 1\} \\
			B &= \{0, 1, \varkappa\}
		\end{aligned} \\
		\xymatrix@=2cm{
			0 \ar[r]^{1-p_\text{о}-p_\text{с}} \ar[rd]^(.3){p_\text{о}} \ar[rdd]^(.7){p_\text{с}} & 0 \\
			1 \ar[r]^{1-p_\text{о}-p_\text{с}} \ar[ru]^(.3){p_\text{о}} \ar[rd]_(.7){p_\text{с}} & 1 \\
			& \varkappa
		}
	\end{gather*}
\end{Def}

\subsection{Пространство Хэмминга}
\begin{Def}
	\textit{Метрика Хэмминга} между двумя векторами $a, b \in A^n$ "--- это количество
	координат, в которых $a$ отличается от $b$.
\end{Def}
\begin{Exercise}
	Доказать, что это метрика: симметричная, обращается в ноль только для равных, есть неравенство треугольника.
\end{Exercise}

\begin{Def}
	\textit{Пространство Хэмминга} "--- это пространство векторов $A^n$ с введённой метрикой Хэмминга.
\end{Def}
Мы в этом пространстве будем работать почти весь оставшийся курс.

\section{Нелинейный код}
\subsection{Определение}
\begin{Def}
	\textit{Нелинейный код} "--- биекция между множеством информационных символов размера $m$
	и некоторым подмножеством $V$ из векторов $A^n$.
	То есть каждому информационному символу однозначно сопоставляем какой-то вектор длины $n$
	из входного алфавита $A=GF(q)$.
\end{Def}
\begin{Rem}
	Название плохое, исторически сложилось как антоним к <<линейным кодам>>.
\end{Rem}
\begin{Rem}
	Скорость такого кода (если у нас информационные символы равновероятны) равна:
	\[
		R = \frac{\log_q m}{n}
	\]
\end{Rem}

\begin{Def}
	\textit{(Минимальное) расстояние кода} "--- это минимальное расстояние между
	векторами, кодирующими разные информационные символы:
	\[
		d \coloneqq \min_{v_1 \neq v_2} d_\text{hamming}(v_1, v_2)
	\]
\end{Def}
\begin{Def}
	\textit{Корректирующая способность $t$ кода} "--- это максимальное число ошибок при передаче
	данных, после которого всё ещё можно без ошибок восстановить информационный символ.
\end{Def}
\begin{Rem}
	Алфавит $A$ у нас может иметь больше двух символов, любое изменение символа на другой мы считаем одной ошибкой.
\end{Rem}
\begin{Exercise}
	Корректирующая способность нелинейного кода равна:
	\[
		t = \left\lfloor \frac{d - 1}{2} \right\rfloor
	\]
\end{Exercise}
\begin{proof}
	Если $d$ нечётное, то любой код будет на расстоянии не больше $\frac{d-1}{2}$ от ровно одного вектора.
	А вот между самыми близкими векторами больше $\frac{d-1}{2}$ ошибок уже допускать нельзя "--- начнём ошибаться.

	Если $d$ чётное, то аналогично.
	Просто в середине между самыми близкими векторами будет неоднозначность.
\end{proof}

Процедура кодирования: надо посмотреть в таблицу и найти там вектор.
Процедура декодирования: просмотреть всю таблицу и найти там ближайший вектор.
Это квадратичная сложность в общем случае, что не очень вдохновляет.
Можно попробовать заоптимизировать или предподсчитать, но всё равно не очень.

Дальше мы, конечно, будем смотреть не на произвольные наборы векторов, а на подпространства линейного пространства, и там будет лучше.

\subsection{Шары Хэмминга}
\begin{Def}
	\textit{Шар Хэмминга с центром в векторе $v_0$ радиуса $r$} "--- это множество всех векторов, отличающихся
	от $v_0$ не более чем в $r$ координатах.
\end{Def}

\begin{lemma}
	Объём шара Хэмминга радиуса $r$ (при $A=GF(q)$ и размере кода $n$):
	\[
		V_r = \binom{n}{0} + (q-1)\binom{n}{1} + (q-1)^2\binom{n}{2} + \dots + (q-1)^r\binom{n}{r}
	\]
\end{lemma}
\begin{proof}
	Просто посчитали вектора на расстоянии ровно 0, ровно 1, \dots, ровно $r$.
\end{proof}
\begin{exmp}
	При $q=3$, $n=5$ шар радиуса $5$ покрывает всё пространство, т.е. $3^5=243$ вектора:
	\begin{align*}
		V_5 &= \binom{5}{0} + 2\binom{5}{1} + 2^2\binom{5}{2} + 2^3\binom{5}{3} + 2^4\binom{5}{4} + 2^5\binom{5}{5} \\
		243 &= 1 + 2\cdot5 + 4\cdot10 + 8\cdot10 + 16\cdot5 + 32\cdot1 \\
		243 &= 1 + 10 + 40 + 80 + 80 + 32
	\end{align*}
\end{exmp}
\begin{Rem}
	Если нарисовать, то эти шарики выглядят как повёрнутые кубики.
\end{Rem}
