% !TeX root = lectures.tex
\subsection{Неполное декодирование}
Полный перебор всех кодовых векторов "--- это \textit{полное декодирование}:
раскодировали всё, что можно.

Математически мы разделяем всё пространство векторов на \textit{решающие области},
в каждой области ровно один кодовый вектор.
В какое пространство попали "--- таков и результат декодирования.
Ещё иногда вводят область без кодовых векторов "--- \textit{отказ от кодирования}.
Технически, конечно, в общем случае нам надо все эти области явно запоминать и перебирать.

\textit{Неполное декодирование} "--- это когда мы забиваем восстанавливать
какие-то ошибки, хотя можем.
Например, в нелинейных кодах можно перебирать не все кодовые векторы,
а обойти шарик радиуса $t$ вокруг сообщения из канала и посмотреть в хэш-таблице:
есть такой вектор в коде или нет.
Если объём шарика меньше, чем у нас кодов, то это быстрее.

Но всё равно на практике это не нужно.

\subsection{Границы}
Напоминание: у нас есть алфавит размера $q$, длина кода $n$,
корректирующая способность $t$, расстояние кода $d$,
количество информационных символов $M$, скорость $R$.

\begin{lemma}[Граница плотной упаковки Хэмминга]
Пусть мы зафиксировали $q$, $n$, $t$.
Тогда имеет место верхняя граница на $M$:
\[
	M \le \frac{q^n}{V_t}
\]
\end{lemma}
\begin{proof}
	Если у нас выбрано $M$ векторов, то вокруг каждого
	есть шар радиуса $t$.
	При этом шары не пересекаются.
	Объём каждого шара известен, суммарный объём шаров
	не превышает объём пространства.
\end{proof}

\begin{Rem}
	Если мы в трёхмерном пространстве (а не в пространстве Хэмминга),
	то равенства быть не может.
	А вот в пространстве Хэмминга иногда (очень иногда) всё-таки
	упаковать оптимально и достичь границу можно.
	Философский смысл "--- <<в пространстве Хэмминга шар "--- это иногда куб>>.
\end{Rem}
\begin{Rem}
	Верхних границ много, мы их все рассмотривать не будем.
\end{Rem}

\begin{lemma}[Нижняя граница Гилберта]
Пусть мы зафиксировали $q$, $n$, $d$.
Тогда существует код, который кодирует хотя бы столько информационных символов:
\[
M \ge \frac{q^n}{V_{d-1}}
\]
\end{lemma}
\begin{proof}
	Выбрали произвольный вектор для центра шарика.
	Взяли его как кодовое слово нелинейного кода.
	Выкинули шарик радиуса $d-1$ (там не могут лежать новые вектора), повторили.
	На каждом шаге выкидываем не больше $V_{d-1}$ точек,
	а все выбранные центры друг от друга лежат на расстоянии
	хотя бы $d$.
\end{proof}
\begin{Rem}
	Может быть код, кодирующий меньше слов, чем нижняя граница Гильберта:
	просто взяли один информационный символ.
\end{Rem}
\begin{Rem}
	Практическо-технической пользы от такой границы и конструкции тоже нет:
	надо запоминать все выбранные вектора.
\end{Rem}

\begin{Rem}
	Верхних границ много, а нижних границ, как ни странно, мало.
	Потому что нижняя граница создаёт код в очень общем случае.
\end{Rem}
\begin{Rem}
	В технике часто надо код с совсем конкретными
	параметрами $q$, $n$, $t$, даже не семейство кодов с разными длинами.
	Чтобы вписаться в стандарт (не свой же принимать "--- это жесть и интриги).
\end{Rem}

\section{Линейные коды}
Частный случай нелинейных кодов.
Мы ограничиваем выбор кодов, зато теперь можно начинать что-то делать на практике.

\begin{Def}
	$(n, k)$-код "--- это $k$-мерное подпространство
	линейного $n$-мерного векторного пространства
	над алфавитом-конечным полем $A=GF(q)$.
	Тут $k$ "--- это параметр кода.
\end{Def}
\begin{conseq}
	Код задаётся базисом из $k$ векторов.
\end{conseq}
\begin{Rem}
	Есть и другие математические ответвления, которыми в этом курсе не занимаемся.
	Например, когда у нас алфавит не то что не поле,
	а даже нет какой-нибудь коммутативности
	(типа алгебры матриц).
	Над ним что-нибудь строим, новые коды, новые границы,
	публикуем математический результат.
	Что делать технически, впрочем, нам пока что неизвестно,
	но вдруг придумают.
\end{Rem}

\begin{Def}
	\textit{Порождающая матрица $G$ $(n, k)$-кода} (generator matrix) "---
	это просто набор из $k$ $n$-мерных векторов ($k$ строк, $n$ столбцов)
\end{Def}
\begin{Rem}
	Хранить $nk$ элементов сильно проще, чем все возможные вектора из
	подпространства.
\end{Rem}

\begin{Rem}
	Далее будем говорить, что у нас на вход кодеру поступают \textit{информационные вектора}
	размерности $k$ из символов алфавита $A$.
	А \textit{информационным символом} будем называть один элемент этого вектора (что отличается от первой лекции).
\end{Rem}
\begin{Def}
	Правило кодирования линейного кода:
	$k$-мерный информационный вектор $a$ из символов алфавита $A$
	кодируется в $aG=C$ ($C$ "--- кодовый вектор, codeword).
\end{Def}
\begin{Rem}
	Теперь понятна скорость кода:
	\[
		R = \frac{k}{n}
	\]
	и количество возможных слов на входе:
	\[
		M = q^k
	\]
	По этому поводу $M$ обычно уже не используют.
\end{Rem}

\begin{Def}
	\textit{Вес Хэмминга для вектора $w$} "--- это количество ненулей в векторе,
	т.е. $d(w, \vec 0)$.
\end{Def}
\begin{lemma}
	Для линейного кода его расстояние равно $\min_{\vec v} d(v, \vec 0)$,
	т.е. расстоянию до ближайшего к нулю вектора.
\end{lemma}
\begin{proof}
	Возьмём два вектора $\vec x$ и $\vec y$.
	Заметим, что $\vec x - \vec y$ тоже лежит в коде.
	Более того, $d(\vec x, \vec y) = d(\vec x - \vec y, \vec 0)$
	(по индукции посмотрели на каждую координату).
	Значит, минимальное расстояние достигается в паре с нулём.
\end{proof}
\begin{Rem}
	Теперь для определения расстояния достаточно перебирать
	не все пары векторов из подпространства (как в общем случае нелинейных векторов),
	а просто все вектора, кроме нуля.
\end{Rem}

\begin{Def}
	\textit{Систематическое кодирование}:
	если $G$ представили в следующем виде (с точностью до перестановки столбцов):
	\[
	k
	\left(
	\overbrace{
	\left\{
	\begin{array}{ccc|ccc}
		1 & 0 & \dots & ? & \dots & ? \\
		0 & 1 & \dots & ? & \dots & ? \\
		\vdots & \vdots & \ddots & ? & \dots & ? \\
	\end{array}
	\right.
	}^{n}
	\right)
	=
	\left(
	\begin{array}{c|c}I_k & A\end{array}
	\right)
	\]
\end{Def}
\begin{Rem}
	Так как у нас линейное пространство, то можно изменять базис,
	не меняя само пространство и его свойства.
	Со строками можно делать линейные преобразования,
	а вот со столбцами ничего нельзя делать.
	Поэтому, строго говоря, единичная матрица может быть не в начале,
	а <<размазана>> по матрице $G$.
	Но в любом случае в таком виде представить можно с точностью до перестановки столбцов.
\end{Rem}
\begin{Def}
	Теперь у нас в любом закодированном слове
	в чистом виде встречаются $k$ информационных символов
	и $r=n-k$ \textit{проверочных} символов.
\end{Def}

\subsection{Матрица чётности}
Обозначим наш $(n, k)$-код (подпространство) как $\mathbb g$.

\begin{Def}
Возьмём ортогональное пространство к $\mathbb g$ "--- $\mathbb g^{\bot}$:
\[
	\mathbb g^{\bot} = \{ x \mid \forall y \in \mathbb g \colon x \cdot y = 0 \}
\]
Тогда можно построить его базис и записать в \textit{проверочную матрицу} $H$
(\textit{матрицу чётности}, parity check matrix) размером $(n-k)\times n$
(тут мы воспользовались знанием о размерности ортогонального пространства,
даже несмотря на то, что у нас поле странное).
\end{Def}
\begin{lemma}
	Вектор-строка $c$ длины $n$ "--- кодовое слово тогда и только тогда, когда
	\[
		c H^\top = \vec 0
	\]
\end{lemma}
\begin{proof}
	\begin{description}
	\item[$\Rightarrow$:]
		Кодовое слово "--- элемент пространства $\mathbb g$.
		По определению ортогонального пространства его скалярное произведение
		с любым вектором из $\mathbb g^\bot$ равно нулю.
		В частности, с базисными векторами $\mathbb g^\bot$, из которых состоит $H$,
		а это в точности левая часть.
	\item[$\Leftarrow$:]
		Мы знаем, что вектор $c$ ортогонален базису из $\mathbb g^\bot$,
		следовательно, он ортогонален любому вектору из $\mathbb g^\bot$.
		А значит, он лежит в ортогональном пространстве ${\mathbb g^\bot}^\bot=\mathbb g$
		(это факт из алгебры, но в учебниках обычно доказывают только для
		полей характеристики ноль).
		Напрямую не доказать, возникает проблема: в $\mathbb g^\bot$
		может быть вектор, который ортогонален сам себе.
	\end{description}
\end{proof}
\begin{Rem}
	Это такой удобный способ проверки принадлежности вектора $c$
	коду: не надо разлагать в базис.
\end{Rem}

\begin{lemma}
	Пусть есть матрица в таком виде (здесь $A$ имеет размер $k\times(n-k)$):
	\[
		G = \left(\begin{array}{c|c} I_k & A \end{array}\right)
	\]
	тогда можно найти к ней проверочную матрицу размером $(n-k)\times n$:
	\[
		H = \left(\begin{array}{c|c} -A^\top & I_{r=n-k} \end{array}\right)
	\]
\end{lemma}
\begin{proof}
	Достаточно проверить, что вектора из $H$ линейно независимы и образуют
	базис подпространства (очевидно), а также что $GH^\top=0$, т.е. все
	из $G $ и $H$ ортогональны друг другу (\TODO почему? У нас же поле странное)
	\TODO
\end{proof}
\begin{Rem}
	Это такой способ построить проверочную матрицу
	без метода ортогонализации Грама-Шмидта.
\end{Rem}

\begin{Rem}
	В пространстве Евклида любой вектор (кроме нулевого)
	лежит либо в подпространстве,
	либо в ортогональном к нему, но не одновременно.

	А в пространстве Хэмминга над $GF(2)$, например, вектор $(0, 1, 1, 0)$
	ортогонален сам себе (так как сумму считаем по модулю два).
	По этому поводу надо действовать аккуратно.
\end{Rem}
\begin{lemma}
	В пространстве Евклида подпространство и ортогональное ему в сумме
	дают полное пространство.
\end{lemma}
\begin{proof}
	Записали матрицу $H$ под $G$, получили матрицу $n\times n$,
	надо показать, что она базис.
	\TODO
	Рассмотрели каждый вектор: он не может быть представлен, как линейная
	комбинация остальных: от своих базисных независим, а чужие в любой линейной
	комбинации дают скаларный ноль, т.е. вектор ортогонален сам себе, упс.
\end{proof}
\begin{Rem}
	Теорема о размерности ортогонального дополнения в конечных
	полях тоже выполняется, но там уже сложно доказывать.
	Книга Блэхута, страница 56.
	В классических книгах по алгебре обычно про конечные поля забывают.
\end{Rem}
\begin{Rem}
	В конечных полях может быть забавно.
	Например, возьмём поле $GF(2)$ и следующее подпространство пространства размерности 6:
	\[
	G=
	\begin{pmatrix}
	0 & 0 & 1 & 1 & 0 & 0 \\
	1 & 1 & 0 & 0 & 0 & 0
	\end{pmatrix}
	\]
	В нём всего четыре вектора: \t{000000}, \t{001100}, \t{110000}, \t{111100}.
	Они же войдут в ортогональное пространство,
	ещё туда войдут вектора \t{000010}, \t{000001}, итого получаем такой базис:
	\[
	H=
	\begin{pmatrix}
	0 & 0 & 1 & 1 & 0 & 0 \\
	1 & 1 & 0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 0 & 0 & 1 \\
	\end{pmatrix}
	\]
	То есть $\mathbb g \subsetneq \mathbb g^\top$, что забавно.
	При этом сумма размерностей сошлась: $\dim \mathbb g + \dim \mathbb g^\top = 2 + 4 = 6$.
\end{Rem}

Декодировать пока не научились, равно как и не было разговора про расстояние.
Но уже можно применять методы из нелинейного кодирования: искать ближайший вектор
или проходить по шару (там особенно удобно проверять, даже не нужна хэш-таблица).
