% !TeX root = lectures.tex
\subsection{Неполное декодирование}
Полный перебор всех кодовых векторов "--- это \textit{полное декодирование}:
раскодировали всё, что можно.

Математически мы разделяем всё пространство векторов на \textit{решающие области},
в каждой области ровно один кодовый вектор.
В какое пространство попали "--- таков и результат декодирования.
Ещё иногда вводят область без кодовых векторов "--- \textit{отказ от кодирования}.
Технически, конечно, это может быть очень долго.

\textit{Неполное декодирование} "--- это когда мы забиваем восстанавливать
какие-то ошибки, хотя можем.
Например, в нелинейных кодах можно перебирать не все кодовые векторы,
а обойти шарик вокруг кода радиуса $t$ и посмотреть в хэш-таблице:
есть такой вектор в коде или нет.
Если объём шарика меньше, чем у нас кодов, то это быстрее.

Но всё равно на практике это не нужно.

\subsection{Границы}
Напоминание: у нас есть алфавит размера $q$, длина кода $n$,
корректирующая способность $t$, расстояние кода $d$,
количество информационных символов $M$, скорость $R$.

\begin{lemma}[Граница плотной упаковки Хэмминга]
Пусть мы зафиксировали $q$, $n$, $t$.
Тогда имеет место верхняя граница на $M$:
\[
	M \le \frac{q^n}{V_t}
\]
\end{lemma}
\begin{proof}
	Если у нас выбрано $M$ векторов, то вокруг каждого
	есть шар радиуса $t$.
	При этом шары не пересекаются.
	Объём каждого шара известен, суммарный объём шаров
	не превышает объём пространства.
\end{proof}

\begin{Rem}
	Если мы в трёхмерном пространстве (а не в пространстве Хэмминга),
	то равенства быть не может.
	А вот в пространстве Хэмминга иногда (очень иногда) всё-таки
	упаковать оптимально и достичь границу можно.
	Философский смысл "--- <<в пространстве Хэмминга шар "--- это иногда куб>>.
\end{Rem}
\begin{Rem}
	Верхних границ много, мы их все рассмотривать не будем.
\end{Rem}

\begin{lemma}{Нижняя граница Гилберта}
Пусть мы зафиксировали $q$, $n$, $d$.
Тогда существует код, который кодирует хотя бы столько информационных символов:
\[
M \ge \frac{q^n}{V_{d-1}}
\]
\end{lemma}
\begin{proof}
	Выбрал произвольный вектор для центра шарика.
	Взяли его как кодовое слово.
	Выкинули шарик, повторили.
	На каждом шаге выкидываем не больше $V_{d-1}$ точек,
	а все выбранные центры друг от друга лежат на расстоянии
	хотя бы $d$.
\end{proof}
\begin{Rem}
	Может быть код, кодирующий меньше слов, чем нижняя граница Гильберта:
	просто взяли один информационный символ.
\end{Rem}
\begin{Rem}
	Практическо-технической пользы от такой границы и конструкции тоже нет:
	надо запоминать все выбранные вектора.
\end{Rem}

\begin{Rem}
	Верхних границ много, нижних границ мало.
\end{Rem}
\begin{Rem}
	В технике часто надо код с совсем конкретными
	параметрами $q$, $n$, $t$, даже не семейство кодов с разными длинами.
	Чтобы вписаться в стандарт (не свой же принимать "--- это жесть и интриги).
\end{Rem}

\section{Линейные коды}
Частный случай нелинейных кодов.
Мы ограничиваем выбор кодов, зато теперь можно начинать что-то делать на практике.

\begin{Def}
	$(n, k)$-код "--- это $k$-мерное подпространство
	линейного $n$-мерного векторного пространства
	над алфавитом-конечным полем $A=GF(q)$.
	Тут $k$ "--- это параметр кода.
\end{Def}
\begin{Rem}
	Есть и другие математические ответвления, которыми в этом курсе не занимаемся.
	Например, когда у нас алфавит не то что не поле,
	а даже нет какой-нибудь коммутативности
	(типа алгебры матриц).
	Над ним что-нибудь строим, новые коды, новые границы,
	публикуем математический результат.
	Что делать технически, впрочем, нам пока что неизвестно,
	но вдруг придумают.
\end{Rem}

Вывод: код задаётся базисом из $k$ векторов.
\begin{Def}
	\textit{Порождающая матрица $G$ $(n, k)$-кода} (generator matrix) "---
	это просто набор из $k$ $n$-мерных векторов ($k$ строк, $n$ столбцов)
\end{Def}
\begin{Rem}
	Хранить $nk$ элементов сильно проще, чем все возможные вектора из
	подпространства.
\end{Rem}

\TODO кажется, есть путаница с первой лекцией, когда у нас
информационный символ сам по себе был вектором.

\begin{Def}
	Правило кодирования линейного кода:
	$k$-мерный информационный вектор $a$ из символов алфавита $A$
	кодируется в $aG=C$ ($C$ "--- кодовый вектор, codeword).
\end{Def}
\begin{Rem}
	Теперь понятна скорость кода:
	\[
		R = \frac{k}{n}
	\]
	и количество возможных слов на входе:
	\[
		M = q^k
	\]
	По этому поводу $M$ обычно уже не используют.
\end{Rem}

\begin{Def}
	\textit{Вес Хэмминга для вектора $w$} "--- это количество ненулей в векторе,
	т.е. $d(w, \vec 0)$.
\end{Def}
\begin{lemma}
	Для линейного кода его расстояние равно $\min_{\vec v} d(v, \vec 0)$,
	т.е. расстоянию до ближайшего к нулю вектора.
\end{lemma}
\begin{proof}
	Возьмём два вектора $\vec x$ и $\vec y$.
	Заметим, что $\vec x - \vec y$ тоже лежит в коде.
	Более того, $d(\vec x, \vec y) = d(\vec x - \vec y, \vec 0)$
	(по индукции посмотрели на каждую координату).
	Значит, минимальное расстояние достигается в паре с нулём.
\end{proof}
\begin{Rem}
	Теперь для определения расстояния достаточно перебирать
	не все пары векторов из подпространства (как в общем случае нелинейных векторов),
	а просто все вектора, кроме нуля.
\end{Rem}

\begin{Def}
	\textit{Систематическое кодирование}:
	если $G$ представили в следующем виде (с точностью до перестановки столбцов):
	\[
	\begin{array}{ccc|cc}
		1 & 0 & \dots & ? & \dots & ? \\
		0 & 1 & \dots & ? & \dots & ? \\
		\vdots & \vdots & \ddots & ? & \dots & ? \\
	\end{array}
	\]
\end{Def}
\begin{Rem}
	Так как у нас линейное пространство, то можно изменять базис,
	не меняя само пространство и его свойства.
	Со строками можно делать линейные преобразования,
	а вот со столбцами ничего нельзя делать.
	Поэтому, строго говоря, единичная матрица может быть не в начале,
	а <<размазана>> по матрице $G$.
	Но в любом случае в таком виде представить можно.
\end{Rem}
\begin{Def}
	Теперь у нас в любом закодированном слове
	в чистом виде встречаются $k$ информационных символов
	и $r=n-k$ \textit{проверочных} символов.
\end{Def}

\subsection{Матрица чётности}
Обозначим наш $(n, k)$-код как подпространство $\mathrm g$.

\begin{Def}
Возьмём ортогональное пространство к $\mathrm g$ "--- $\mathrm g^{\bot}$.
Тогда можно построить его базис и записать в \textit{проверочную матрицу} $H$
(\textit{матрицу чётности}, parity check matrix).
\end{Def}
\begin{lemma}
	Для любого кодового слова $c$:
	\[
		c H^\top = 0
	\]
	А для не-кодовых слов это неверно.
\end{lemma}
\begin{proof}
	Кодовое слово "--- элемент пространства $\mathrm g$.
	По определению.
	\TODO
\end{proof}
\begin{Rem}
	Это такой удобный способ проверки принадлежности вектора $c$
	коду: не надо разлагать в базис.
\end{Rem}

\begin{lemma}
	Пусть есть матрица в таком виде:
	\[
		G = [ I_k | A ]
	\]
	тогда можно найти к ней проверочную матрицу:
	\[
		H = [ -A^\top | I_r ]
	\]
\end{lemma}
\begin{proof}
	\TODO
\end{proof}
\begin{Rem}
	Это такой способ построить проверочную матрицы
	без метода ортогонализации Грама-Шмидта.
\end{Rem}

\begin{Rem}
	В пространстве Евклида любой вектор (кроме нулевого)
	лежит либо в подпространстве,
	либо в ортогональном к нему, но не одновременно.

	А в пространстве Хэмминга над $GF(2)$, например, вектор $(0, 1, 1, 0)$
	ортогонален сам себе (так как сумму считаем по модулю два).
	По этому поводу \TODO
\end{Rem}
\begin{lemma}
	В пространстве Евклида подпространство и ортогональное ему в сумме
	дают полное пространство.
\end{lemma}
\begin{proof}
	Записали матрицу $H$ под $G$, получили матрицу $n\times n$,
	надо показать, что она базис.
	\TODO
	Рассмотрели каждый вектор: он не может быть представлен, как линейная
	комбинация остальных: от своих базисных независим, а чужие в любой линейной
	комбинации дают скаларный ноль, т.е. вектор ортогонален сам себе, упс.
\end{proof}
\begin{Rem}
	Теорема о размерности ортогонального дополнения в конечных
	полях тоже выполняется, но там уже сложно доказывать.
	Книга Блэхута, страница 56.
	В классических книгах по алгебре обычно про конечные поля забывают.
\end{Rem}

Декодировать пока не научились, равно как и не было разговора про расстояние.
Но уже можно применять методы из нелинейного кодирования: искать ближайший вектор
или проходить по шару (там особенно удобно проверять, даже не нужна хэш-таблица).
