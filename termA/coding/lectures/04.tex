На прошлой лекции начали формулировать алгоритм декодирования:

\begin{enumerate}
\item
	Вычисление \textit{синдрома}: $S(b)=bH^\top$
\item
	Восстановление предполагаемой ошибки из синдрома: $S \to \hat e$
	($\hat e$ вместо $e$, потому что нет гарантий, что это
	реальная ошибка, мы попытались угадать)
\item
	Восстановление кодового слова: $\hat c = b - \hat e$
\end{enumerate}

Самый загадочный шаг "--- второй, потому что у разных ошибок может совпадать синдром.
Например, векторов ошибок у нас различных $2^n$, а синдромов "--- $2^{n-k}$, что сильно меньше.

\begin{theorem}
	Пусть есть две ошибки: $e_1\neq e_2$, причём
	$W(e_1), W(e_2) \le t$ (где $t$ "--- корректирующая способность кода).
	Тогда $S(e_1) \neq S(e_2)$.
\end{theorem}
\begin{proof}
	\begin{align*}
	S(e_1) &\neq S(e_2) \\
	e_1H^\top &\neq e_2H^\top \\
	(e_1-e_2)H^\top &\neq 0 \\
	e_1 - e_2 \in J \\ \text{кодовое слово} \\
	W(e_1 - e_2) &= d(e_2, e_1) \le d(e_2, 0) + d(e_1, 0) = 2t < d
	\end{align*}
	Противоречие: получили кодовый вектор веса меньше $d$.
\end{proof}

Теперь можно перебрать все вектора ошибки всом до $t$,
построить синдромы, отсортировать, получаем алгоритм декодирования.

Итого в первом шаге сложность $\O(n^2)$, во втором "--- $\O(n)$ (поиск в таблице).
А вот табличка экспоненциальная, памяти надо много.

\begin{Rem}
	Таблица полная $\iff$ код совершенный,
	т.е. достигается граница Хэмминга:
	\[
		\sum_{i=0}^t \binom{n}{i} (q-1)^i = q^r = q^{n-k}
	\]
\end{Rem}
\begin{proof}
	\TODO
\end{proof}
\begin{Rem}
	А если таблица не полная, то мы иногда можем отказываться
	от кодирования.
	Но если отказываемся, то у нас точно произошло больше $t$ ошибок.
\end{Rem}

\subsection{Стандартное расположение}
Нарисуем табличку из векторов (ширина $q^k$).
Что-то вроде картинки с факторгруппами: в первой строке записали $q^k$ кодовых слов.
В следующей "--- выбрали какой-то вектор $e_0$, которого не было в первой строчке,
записали $e_0+c_0, e_0+c+1, \dots$, получили вторую строчку.
Во второй выбрали вектор, которого нет среди последних $2q^k$, и так далее.

По построению у нас каждый вектор встретится хотя бы один раз
(так как есть нулевое кодовое слово вроде $c_0$).

\begin{Def}
	Каждая строчка "--- смежный класс кода, обозначается как $e_i + J$.
	Его \textit{лидер} (или \textit{образующая}) "--- $e_i$.
\end{Def}

\begin{lemma}
	Два смежных класса либо не пересекаются, либо совпадают.
\end{lemma}
\begin{proof}
	Взяли $e_i+c_j=e_k+c_l$.
	Перенесли: $e_i=e_k+(c_l-c_j)$, справа получили кодовое слово $c$.
	Тогда $e_i$ принадлежит смежному классу $e_k$, упс.
	Аналогично, $e_k$ принадлежит смежному классу $e_i$.
	\TODO
\end{proof}

Довольно удобно каждый раз выбирать в качестве лидера вектор наименьшего веса
(сейчас поймём почему).

Если у нас корректирующий код способностью $t$, то расстояние между
двумя векторами внутри смежного класса не меньше $t$ (\TODO).

\begin{theorem}
	Синдромы у всех смежных классов определены однозначно и отличаются
\end{theorem}
\begin{proof}
	У всех внутри строчки совпадают: \TODO

	У разных лидеров синдромы отличаются (иначе перенесём): \TODO
\end{proof}

В табличке получили все $q^n$ векторов в $q^r$ строчках.
Отсюда то, как можно смотреть на таблицу: мы по синдрому выясняем только
строчку, в которой лежит вектор ошибки.
А вот какой выбрать вектор ошибки из строчки "--- надо уже думать,
выбираем лидера.

\begin{theorem}
	Если в качестве лидера в каждом смежном классе выбрать
	вектор наименьшего веса, то синдромное декодирование
	совпадёт с декодированием по минимуму расстояния Хэмминга.
\end{theorem}
\begin{proof}
	Пусть синдромное декодирование нашло $b=c_1+e_1$,
	где $e_1$ "--- какой-то лидер.
	Пусть есть другой кодовый вектор $c_2$.
	Тогда $b=c_2+e_2$.
	Отсюда $e_2=b-c_2=c_1+e_1-c_2=e_1+(c_1-c_2)$,
	т.е. $e_2$ лежит в смежном классе $e_1$ (и наоборот).

	А ещё мы знаем, что $d(c_2, b) < d(c_1, b)$ (\TODO):
	\[
		W(e_2) = d(c_2, b) < d(c_1, b) = W(e_1)
	\]
\end{proof}
\begin{Rem}
	Это более сильное утверждение: мы потенциально может декодировать даже
	больше $t$ ошибок.
	Это \textit{полное} декодирование, а раньше было только \textit{неполное}.
\end{Rem}

\subsection{Вероятность ошибки}
Пусть у нас ДСК (двоично-симметричный канал) с вероятностью ошибки $p$.
Тогда мы в полном декодировании исправляем любую ошибку, которая является лидером смежного класса.
Обозначим за $A_i$ количество лидеров веса $i$.
Тогда вероятность ошибки:
\[
	P_{err} = 1 - \sum_{i=0}^n A_i p^i (1-p)^{n-i}
\]

А при неполном декодировании мы точно значем количество лидеров веса $i$
(\TODO), поэтому:
\[
	P_{err} = 1 - \sum_{i=0}^t \binom{n}{i} p^i (1-p)^{n-i}
\]

\section{Циклические кода}
Циклические коды "--- это когда мы привлекли алгебру многочленов вместо линейной алгебры.

Будем рассматривать кольца $GF(q)[x]$ (многочлены с коэффициентами из конечного поля)
и они же по модулю $x^n-1$ (или, что то же самое, заменяем везде $x^n$ на единицу), это кольцо $R_n$.
И работать в этих алгебрах.
При этом требуем $(n,q)=2$, иначе потом будет неудобно.

Циклический код "--- это такой код, что циклический сдвиг любого кодового слова тоже кодовое слово.

Например: $(c_0, c_1, \dots, c_{n-1}) \sim \sum_{i=0}^{n-1} c_i\cdot x^i$.
Можно говорить, что кодовое слово "--- это и вектор, и многочлен.

\begin{theorem}
	Циклический сдвиг соответствует умножению многочлена на $x$ в кольце $R_n$.
\end{theorem}
\begin{Rem}
	Циклический сдвиг в другую сторону "--- это умножение на $x^{n-1}$ в том же кольце
	($n-1$ циклический сдвиг).
\end{Rem}
