\chapter{AVL, Treap, Impicit key, Persistent search tree}

\section{AVL-дерево}

\subsection{Основное}

1962, Адельсон"---Вельский"---Ландис.

AVL-дерево "--- это BST, у которого поддерживается инвариант, что разница высот левого и правого поддерева
для каждой вершины не превосходит 1.
Зная это, можно получить следующие ограничения на размер дерева:
\begin{gather*}
	size[h] \ge size[h - 1] + size[h - 2] \\
	2^h - 1 \ge size[h] \ge F_n
\end{gather*}
Из данного соотношения видно, что $h = O(\log n)$.

\subsection{Балансировка}

Чтобы балансировать наше дерево, мы будем поступать так: как только мы как-то изменили вершину, смотрим,
какие высоты у ее левого и правого сыновей.
Пока они различаются более, чем на 1, то совершаем один из четырех поворотов:
Right rotation, Left rotation, Big right rotation и Big left rotation.

\begin{center} \includegraphics[width=6in]{left_right_rotation.png} \end{center}

Из картинки видно, что правый поворот мы совершаем тогда, когда высота левого поддерева больше, чем высота правого.
Тем не менее, и этот случай делится на два.
Right rotation мы совершаем только в том случае, если $P.h > C.h \land A.h \ge B.h$
(аналогично для (малого) левого поворота "--- он происходит, тогда, когда $Q.h > A.h \land C.h \ge B.h$).

Остались случаи, когда $P.h > C.h \land A.h < B.h$ (и симметричный для Big Left Rotation).
Здесь мы будем применять Big right rotation.

\begin{center} \includegraphics[width=6in]{bigright.jpg} \end{center}

То есть поворот здесь происходит, если $k_1.h > D.h \land k_2.h > A.h$.
Аналогично делается Big left rotation.
Теперь можем заметить, что Right rotation "--- это вращение вокруг ребра $P$"---$Q$.
А Big right rotation "--- вокруг ребер $k_1$"---$k_2$ (обычный left rotation), а потом $k_1$"---$k_3$ (обычный right rotation).

\subsection{Реализация}

Код этого дела будет выглядеть примерно так:
\begin{cppcode}
Node* right_rotation(Node* n) {
	Node* left = n->l;
	n->l = left->r;
	left->r = n;

	upd(left); upd(n);

	return left;
}
\end{cppcode}
\texttt{upd} "--- функция, в которой пересчитываются высоты.

\begin{cppcode}
Node* make_balanced(Node* n) {
	size_t lh = get_h(n->l);
	size_t rh = get_h(n->r);

	while (abs(lh - rh) >= 2) {
		if (lh > rh) {
			if (get_h(n->l->l) >= get_h(n->l->r)) {
				n = right_rotation(n);
			} else { //Это Big right rotation
				n->l = left\_rotation(n->l); // поворот вокруг k1-k2
				n = right\_rotation(n); //поворот вокруг k1-k3
			}
		} else {
			if (get_h(n->r->r) >= get_h(n->r->l)) {
				n = left_rotation(n);
			} else { //А это Big left rotation
				n->r = right_rotation(n->r);
				n = left_rotation(n);
			}
		}

		lh = get_h(n->l);
		rh = get_h(n->r);
	}

	upd(n);
	return n;
}
\end{cppcode}

\subsection{Удаление}

Как делать добавление понятно "--- так же, как и в просто в BST, только надо будет следить за высотами поддеревьев.
Удаление легче всего делать следующим образом: свопнемся с первым большим себя элементом (Right) из нашего поддерева.
Теперь себя можно честно удалить, подвесив своего правого сына (если таковой был) вместо себя к нашему предку.

\subsection{Ссылки}

На Хабре есть хорошая статья про AVL деревья:
\href{http://habrahabr.ru/post/150732/}{http://habrahabr.ru/post/150732/}

\section{Неявный ключ}

Пусть нам задан некоторый массив чисел и мы хотим по нему построить BST так, чтобы при симметричном обходе
(т.е. просто если посмотреть на его ключи слева-направо) все его ключи были в том же порядке, что и в изначальном массиве.

По-идее, мы могли бы просто добавлять пары $(i, a_i)$ и интерпретировать $i$ как $x$, по которому и строим BST,
а $a_i$ "--- просто некоторая информация, хранящаяся в вершине.
В принципе да, мы решили поставленную задачу "--- ключи идут в нужном порядке.

Теперь заметим, что на самом деле хранить сам ключ $i$ в вершине нам совершенно необязательно "--- нам просто нужно знать для каждой
вершины сколько вершин левее нее. Если больше, чем $i$, то наша вершина должна быть левее текущей. Если меньше, то правее.

Получаем, что мы можем вообще не хранить $x$ для вершин. Храним только поле \verb'left_size'.
Собственно, в этом и заключается идея неявного ключа. Когда мы пользуемся <<явным>> ключом, мы как бы говорим,
что нам не важен относительный порядок элементов, а важно лишь кто кого больше.
Если мы используем "неявный" ключ, мы не сможем сделать так, чтобы $a_i$ были расположены в соответствии с тем,
как они между собой соотносятся, но зато мы всегда знаем их порядок в массиве.
То есть неявный ключ, в отличие от явного, применим тогда, когда нам важен порядок, в котором заданы элементы,
и нам абсолютно все равно, как соотносятся \texttt{data} ($a_i$).

Поиск $k$-го элемента в этом дереве будет выглядеть примерно так:
\begin{cppcode}
int find(Node* n, int k) {
	if (n->lsize == k)
		return n->data;

		if (n->lsize > k)
			return find(n->l, k);
		else
			return find(n->r, k - n->lsize);
		}
\end{cppcode}

\section{Персистентность}

Чтобы сделать наше дерево персистентным (т.е. таким, что мы всегда как-то храним все его предыдущие версии и можем к ним откатиться),
надо просто немного изменить добавление (и удаление) вершины.
Вместо того, чтобы делать какие-то изменения в текущей вершине, мы будем создавать ее точную копию и изменять именно ее.
Возвращать из функции нужно тоже именно эту новую вершину.
Дальше нужно просто хранить ссылки на все корни.

Вообще, любое сбалансированное дерево можно сделать персистентным, увеличив время работы в $\log n$ раз.

Кстати, заметим, что мы только что научились делать персистентный массив "--- строим персистентное BST по неявному ключу
на элементах этого массива.

\section{Декартово дерево}

\subsection{Основное}

1989, Cecilia R. Aragon, Raimund Seidel.

Декартово дерево (Treap, Дерамида, Курево) "--- BST, в котором выполняется следующее свойство "--- оно является BST по нашим $x$-ам
и в то же время кучей по $y$-ам.
То есть пусть нам дано множество пар:
\[ \left<x_1, y_1\right>, \left<x_2, y_2\right>, \dots, \left<x_n, y_n\right> \]
где $x_n$ и $y_n$ "--- какие-то случайные перестановки. Докажем два факта:
\begin{enumerate}
\item
	Существует ровно один Treap, построенный на этих ключах.

	Это более-менее очевидное утверждение.
	Просто заметим, что есть какой-то наибольший $y_k$. Он должен быть в корне Treap'a.
	Заметим теперь, что $x_k$ поделил нам задачу на две независимых, в которых
	мы так же единственным образом восстанавливаем корни, которые будут
	детьми $\left<x_k, y_k\right>$, и т.д.

\item
	Если $y_1, y_2, ..., y_n$ "--- случайная перестановка, то матожидание высоты Treap'a "--- $O(\log n)$.

	Доказательство примерно такое же, как и для qsort'a.
	Просто теперь на каждом отрезке у нас случайным образом выбирается корень
	(т.к. $y$-ки образуют случайную перестановку) поддерева и задача разделяется на две независимых.

	На практике, конечно, никто не генерирует рандомные перестановки "--- просто каждый раз,
	создавая очередную вершину, генерируем ей рандомный $y$.
\end{enumerate}

\subsection{Split и Merge}

У декартова дерева есть 2 основных операции "--- split и merge.
Split по заданному ключу делит дерево на два "--- в первом из них ключи меньше заданного, во втором больше.
Merge принимает два дерева, первое из которых меньше второго (все $x$ из первого меньше, чем $x$ из второго) и делает из них одно.

\begin{cppcode}
Treap Merge(Node* n1, Node* n2) {
    if (!n1) {
        return n2;
    }
    if (!n2) {
        return n1;
    }

    if (n1->y < n2->y) {
        Treap tmp = Merge(n1->r, n2);
        n1->r = tmp.root;

        return Treap(n1);
    } else {
        Treap tmp = Merge(n1, n2->l);
        n2->l = tmp.root;

        return Treap(n2);
    }
}
\end{cppcode}

\begin{center} \includegraphics[width=6in]{treap-merge.png} \end{center}
\begin{center} \includegraphics[width=6in]{treap-merge1.png} \end{center}

\begin{cppcode}
TreapPair split(Node* n, int x) {
    if (!n) {
        return mp(Treap(NULL), Treap(NULL));
    }

    if (n->x < x) {
        TreapPair tmp = split(n->r, x);
        n->r = tmp.first.root;

        return mp(Treap(n), tmp.second);
    } else {
        TreapPair tmp = split(n->l, x);
        n->l = tmp.second.root;

        return mp(tmp.first, Treap(n));
    }
}
\end{cppcode}

\begin{center} \includegraphics[width=6in]{treap-split.png} \end{center}

\subsection{Insert/Delete}

Теперь понятно, как должны выглядеть insert и delete.
Insert "--- разделил дерево на 2 "--- меньшее и большее нашего $x$-a.
Теперь примержили к первой половине наш $x$, после чего этот результат примержили ко второй половине.
Аналогично Delete "--- дважды разрезаем, чтобы получить нужную вершину, после чего мержим два других дерева без этой вершины.
То есть
\[ Insert = Split + Merge + Merge \]
\begin{center} \includegraphics[width=6in]{treap-insert.png} \end{center}
\[ Delete = Split + Split + Merge \]
\begin{center} \includegraphics[width=6in]{treap-delete.png} \end{center}

Теперь ускорим \texttt{Insert} в 2 раза.
Сейчас мы делаем два спуска по дереву (один из Merge'й на самом деле работает за $O(1)$).
Можно делать только 1: идем по дереву поиска, пока не получим тот момент, \cpp'n->y > y'
(если считать, что минимум $y$-ов должен быть в корне).
Теперь Split можно запускать уже только с этого места.
После чего создаем новую вершину и подвешиваем за нее те два дерева, что вернул Split.

\subsection{Ссылки}

На Хабре есть отличный цикл из трех статей про декартово дерево (эти замечательные картинки взяты оттуда):
\href{http://habrahabr.ru/post/101818/}{http://habrahabr.ru/post/101818/}

\section[Случай равных x-ов]{Случай равных $x$-ов}

\begin{enumerate}
	\item Можно хранить в вершине не только $x$, но и $count$ "--- кол-во этих $x$-ов.
	\item Хранить пару $\left<x, i\right>$ "--- не только $x$, но и его порядковый номер.
	\item Говорим, что каждый следующий $x$ чуть больше предыдущего (т.е. просто в split'e все равные нашему $x$-ы кидаем в левое дерево).
\end{enumerate}
