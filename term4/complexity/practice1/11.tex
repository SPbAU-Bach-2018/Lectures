\chapter{Занятие 20.04.2016}

\section{Разбор задач}
\problem{40}
	Разбирал Дима Розплохас.

	Заметим, что одно включение верно всегда $\RP \subset \NP$,
	так как если язык лежит в $\RP$, то для него много подсказок $\Ra$
	есть хотя бы одна (что является требованием $\NP$).

	Теперь докажем, что $\NP \subset \RP$ в предположении задачи.
	Для этого докажем, что $SAT \in \RP$, этого будет достаточно "---
	любую задачу можно сначала точно свести к $SAT$ и решить его в $\RP$.
	По условию задачи у нас есть алгоритм, который решает $SAT$ с двухсторонней ошибкой.
	Мы умеем понижать вероятность ошибки на входе до $\frac{1}{2^n}$ с сохранением полиномиального времени работы,
	получим алгоритм $A$.
	Хотим получить алгоритм, который:
	\begin{enumerate}
		\item Если формула невыполнима, то он всегда выдаёт ответ <<невыполнима>>.
		\item Если формула выполнима, то он выдаёт ответ <<выполнима>> с вероятностью хотя бы $\sfrac 23$.
	\end{enumerate}
	В частности, если алгоритм говорит <<выполнима>>, то формула действительно выполнима.

	Наш алгоритм будет работать на формуле $\phi$ так: мы сначала запускаем $A(\phi)$.
	Если ответ "--- <<невыполнима>>, то отвечаем так же.
	Если выполнима, то подставляем два возможных значения $x_1$ и проверяем выполнимость формул
	с подставленным $x_1=0$ и $x_1=1$.
	Если хотя бы одна формула с подстановкой получилась выполнима, то мы идём туда и подставляем следующую переменную.
	В конце-концов мы либо скажем, что формула невыполнима, либо дойдём до конца, получим какой-то набор переменных.
	Его мы уже явно проверим "--- является ли он выполняющим для $\phi$ или нет.
	Если действительно является "--- возвращаем <<да>>, если нет "--- возвращаем <<нет>>.
	Таким образом наш алгоритм может ошибиться лишь так: сказать, что формула невыполнима, когда она на самом деле выполнима.

	Осталось оценить вероятность этой ошибки.
	Нарисуем дерево принятия решений алгоритмом, оно имеет глубину $n+1$, т.е. в той ветке,
	в которой нашлось решение, $A$ запускался $n+1$ раз (на $\phi$, на $\phi[x_1=\dots]$ и так далее).
	Алгоритм ошибается, если в этой конкретной ветке хотя бы один запуск $A$ ошибся.
	Вероятность ошибки на каждом шаге не более $\frac{1}{2^n}$, тогда можно оценить вероятность ошибки хотя бы на
	одном шаге как $\frac{n+1}{2^n}$, что при больших $n$ меньше константы.
	\begin{Rem}
		Перемножать вероятности отсутствия ошибки так просто было бы нельзя "--- могли бы спросить про независимость событий.
		Но тут она есть, слава богу.
	\end{Rem}

\problem{51}
	Разбирал Игорь Лабутин.

	В одну сторону очевидно: $\MA_1 \subseteq \MA$ (так как если допустима двухсторонняя ошибка,
	то односторонняя тоже).

	Теперь пусть есть язык $L$ и мы хотим избавиться от ошибки в одну сторону.
	Раз $L \in \MA$, то мы знаем следующее:
	\[ x \in L \iff \exists w \colon \Pr\limits_r[A(x, w, r)] \ge \frac 23 \]
	Давайте изменим протокол.
	Будем просить, чтобы Мерлин выдавал не только $w$, но и вспомогательный набор случайных строк $k_1, k_2, \dots, k_l$.
	Потребуем, чтобы протокол удовлетворял следующим свойствам:
	\begin{align*}
		x \in L &\Ra \exists w, k_1, \dots, k_l \colon \forall k \colon \left(\Lor_{i=1}^l A(x, w, k \oplus k_i)\right) = 1 \\
		x \notin L &\Ra \forall w, k_1, \dots, k_l \colon \Pr\limits_k \left[ \left(\Lor_{i=1}^l A(x, w, k \oplus k_i)\right) = 1\right] \le \frac 1 3 \\
	\end{align*}
	\begin{Rem}
		Физический смысл: есть случайные биты, на которых работаем плохо.
		Из каждой точки хотя бы одна стрелочка ($\oplus k_i$) ведёт за пределы бит, на которых не работаем.
	\end{Rem}
	Давайте сначала понизим ошибку $A$ так, чтобы она была не более $\frac{1}{2^{n^2}}$.
	\TODO смёржить с тем, что ниже/сослаться туда?

	Давайте сначала покажем первое условие.
	Пусть $x \in L$.
	У нас была подсказка Мерлина $w$ такая, что вероятность ошибки $A$ маленькая.
	Скажем, что она будет частью нужной подсказки.
	\begin{assertion}
		Если выбрать случайные числа $k_1, k_2, \dots, k_l$, то вероятность того, что
		этот набор не подойдёт, будет меньше единицы.
	\end{assertion}
	\begin{proof}
		Если $k_i$ выбраны случайно и набор не подходит, то это значит, что:
		\begin{gather*}
			\lnot \left(\forall k \colon \left(\Lor_{i=1}^l A(x, w, k \oplus k_i)\right) = 1\right) \\
			\exists k \colon \lnot \left(\Lor_{i=1}^l A(x, w, k \oplus k_i)\right) = 1 \\
			\exists k \colon \left(\Lor_{i=1}^l A(x, w, k \oplus k_i)\right) = 0 \\
			\exists k \colon \Land_{i=1}^l (A(x, w, k \oplus k_i) = 0)
		\end{gather*}
		Давайте оценим вероятность этого события при случайных $k_i$.
		
		Давайте сначала оценим вероятность того, что при фиксированном $k$ случайный набор будет плохой:
		\[
			\Pr\limits_{k_i} [ (\Land_i A(x, w, k \oplus k_i) = 0 ] < \frac{1}{2^{n^2l}}
		\]
		Это так, потому что перемножили вероятность того, что каждое из $k_i$ будет плохое при фиксированном $k$.
		Эти события независимы, так как $k_i$ независимы.

		Теперь оценим вероятность того, что при случайном наборе найдётся плохое $k$.
		То есть произошло хотя бы одно из $2^{|k|}$ событий <<случайный набор оказался плохим для фиксированного $k$>>.
		То есть оценка "--- $\frac{2^{|k|}}{2^{n^2l}}$, можно выбрать $l=|k|$ и получим хорошую оценку.

		\textbf{НЕ НУЖНО?}

		Она не больше, чем вероятность, если мы для каждого $k_i$ разрешим выбирать $k$ отдельно,
		а не одно на всех.

		Мы знаем, что при фиксированном $k$ (например, при $k=0$ это определение):
		\[
			\Pr\limits_{k_i} [ A(x, w, k \oplus k_i) = 0 ] < \frac{1}{2^{n^2}}
		\]
		Давайте теперь посмотрим, что происходит при фиксированном $k_1$:
		\[
			\Pr\limits_{k_1} [ \exists k \colon A(x, w, k \oplus k_i) = 0 ] =
			\Pr\limits_{k_1} [ \Lor_k A(x, w, k \oplus k_i) = 0 ] =
			\le \sum_k \Pr\limits_{k_1} [ A(x, w, k \oplus k_i) = 0 ] <
			\frac{2^{|k|}}{2^{n^2}}
		\]
		\textbf{/ НЕ НУЖНО}

		\textbf{НЕ НУЖНО}

		Значит, вероятность того, что хотя бы одно из $k_i$ не подойдёт, оценивается так:
		\[ \frac{l \cdot 2^{|k|}}{2^{n^2}} \]
		\begin{Rem}
			Последняя строчка не нужна "--- $|k|$ и так может быть $>n^2$ (так как мы ошибку понижали,
			и $k$ может быть произвольно большим).
			Надо что-то говорить про независимость, чтобы можно было не домножать на $l$, а возводить в степень $l$.
		\end{Rem}

		\textbf{/НЕ НУЖНО}

		\textbf{=====}

		Теперь посмотрим, что происходит, если $x \notin L$.
		У нас при случайном выборе $k$ провал будет тогда и только тогда, когда
		провал хотя бы при одном $k_i$.
		Вероятность каждого провала (по случайным $k$) есть $\frac{1}{2^{n^2}}$ "--- мы понижали ошибку,
		а при фиксированном $k_i$ величина $k \oplus k_i$ тоже случана.
		Просуммировали, получили $\frac{l}{2^{n^2}}$.
		Так как $l=|k|=poly(n)$, то при больших $n$ вероятность хороша.

		\begin{Rem}
			В первом случае есть большой коврик с единицами, а ещё "--- маленький кусок из нулей.
			Мы берём коврик с единичками и сдвигаем его в $l$ направлениях, наверняка же затрём целиком кусок из нулей.

			Во втором случае есть большой коврик с нулями, а ещё "--- маленький кусок из единиц.
			Даже если мы этот кусок сдвинем в $l$ направлениях, то он всё еще останется маленьким.
		\end{Rem}
	\end{proof}
	Из утверждения сразу следует, что у нас есть хотя бы один подходящий набор.
	Его Мерлин и будет присылать вместе с $w$.

	\begin{Rem}
		Что-то очень похожее было на лекции, когда $\BPP \subseteq \Sigma_2$.
	\end{Rem}

\problem{52}
	Разбирал Игорь Лабутин.

	\[ \Sigma_2 = \NP^{\co\NP} = \NP^{\NP} \]
	Воспользуемся знанием $\MA=\MA_1$ и покажем, что $\MA_1 \subset \Sigma_2$.

	Берём язык $L \in \MA_1$, для него есть машина-Артур $V$.
	Возьмём оракула, который говорит: верно ли, что существуют случайные биты, на которых $V$ говорит ноль
	(при фиксированной подсказке Мерлина).
	Потом нижняя машина просто угадывает подсказку Мерлина и проверяет, что она хороша.
	\begin{Rem}
		Если пытаться решать с $\MA$, а не $\MA_1$, то нам бы потребовалось повторить предыдущую задачу.
	\end{Rem}

\section{Подсказки}
\subproblem{49}{в}
	Мы знаем, что число полных паросочетаний "--- это перманент, но считать его не умеем.
	Зато умеем считать определитель.

	Давайте для данного случая попробуем придумать матрицу, определитель которой даст нам количество полных паросочетаний.
	Возьмём шахматную доску.
	Мы знаем, что ей соответствует двудольных граф (чёрные клетки и белые клетки).
	Горизонтальному ребру в матрице смежности сопоставим вес 1, вертикальному ребру "--- вес $i$.
	А потом надо понять, что ответ получится равен $\sqrt{|\det M|}$ (возможно, под корнем "--- квадрат).
	Способов доказывать много.
	Можно, например, рассмотреть $M^2$ и понять, что там единички ведут себя совсем хорошим образом.
	Называется это всё <<матрица Кастеляйна>> (<<Kasteleyn matrix>>).
