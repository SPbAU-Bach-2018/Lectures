\setcounter{section}{16}
\section{Билет 17}
\subsection{Рандомизированные машины}
	Случайные биты поступают независимо от входа и алгоритма.
	Потенциально битов бесконечно много (но мы используем только конечное число).
	Считаем, что каждый бит равновероятно 0 или 1.
	Можно и по-другому считать, но мы так делать не будем.

	Можно воспринимать случайные биты либо как дополнительную ленту у МТ
	(тогда будет рандомизированная МТ), либо как дополнительный вход у МТ, на который накладываются какие-то ограничения.
	\begin{Rem}
		Если МТ не может использовать столько же памяти, сколько времени, то она может только читать ленту со случайными битами,
		а головку двигать только вправо.
		Это сделано чтобы нельзя было <<возвращаться в прошлое>> и использовать старые случайные биты вместо памяти.
	\end{Rem}

	Рандомизированные алгоритмы с ответом <<да/нет>> могут ошибаться в две стороны, могут в одну,
	могут не ошибаться (тогда точно рандомизируется время, иначе зачем нам рандомные биты вообще).

\subsection[RP]{$\RP$}
	\begin{Def}
		$L \in \RP$ (односторонняя ошибка), если имеется п.о. п.п. отношение $R$ такое, что для любого входа $x$:
		\begin{enumerate}
			\item $x \notin L \Ra \forall w \colon (x, w) \notin R$
			\item $x \in L \Ra \Pr_w \{ (x, w) \in R \} > \frac 12$
		\end{enumerate}
	\end{Def}
	\begin{Rem}
		Отличие от $\NP$ в том, что там нам требовалась хотя бы одна подсказка, а тут их должно быть хотя бы половина.
	\end{Rem}
	\begin{Rem}
		Если алгоритм для языка из $\RP$ говорит <<$x \in L$>>, то он не врёт "--- нашлась хотя бы одна подсказка.
		А вот если говорит <<$x \notin L$>>, то это ещё неточно.
	\end{Rem}

	\begin{assertion}
		Мы можем считать, что машина всегда получает на вход фиксированное полиномиальное число случайных бит "--- $p(n)$ ($n$ "--- длина входа).
	\end{assertion}
	\begin{proof}
		Во-первых, понятно, что есть такой $p(n)$, что машина всегда использует не более $p(n)$ бит, достаточно взять $p$, ограничивающий время работы машины.
		Во-вторых заметим, что если в произвольный момент работы машины вставить чтение и игнорирование случайного бита, то это не повлияет на вероятности
		выдать какой-либо ответ "--- мы лишь расщепили один случай на два с таким же вердиктом и с вдвое меньшими вероятностями.
		Тогда давайте скажем, что перед тем, как машина выдаёт ответ, она считывает и игнорирует ещё $p(n)-k$ случайных бит, где $k$ "--- число использованных случайных бит.
	\end{proof}
	\begin{conseq}
		\[ \Pr_w \{ (x, w) \in R \} = \frac{|\{ w \mid (x, w) \in R \}|}{2^{p(n)}} \]
	\end{conseq}

	\begin{theorem}
		В $\RP$ можно понижать ошибку, т.е. для любого языка $L \in \RP$ и полинома $p(n)$ есть п.о. п.п. отношение $R$ такое, что:
		\begin{enumerate}
			\item $x \notin L \Ra \forall w \colon (x, w) \notin R$
			\item $x \in L \Ra \Pr_w \{ (x, w) \in R \} > 1 - \frac 1{2^{p(|x|)}}$
		\end{enumerate}
	\end{theorem}
	\begin{proof}
		Давайте возьмём отношение $R$ из определения $\RP$ и повторим проверку $p(|x|)$ раз (это всё ещё полином).
		Вернем ответ <<$x \in L$>> только если хотя бы одна проверка вернула <<да>>.
		Тогда если $x \notin L$, то мы <<да>> никогда не вернём.
		А если $x \in L$, то чтобы ошибиться, нам надо, чтобы каждая из проверок ошиблась,
		это произойдёт с вероятностью не больше $(\sfrac12)^{p(|x|)}$.
	\end{proof}
	\begin{conseq}
		В определении $\RP$ константа $\frac 12$ взята с потолка, можно брать любую, отделённую от единицы и нуля "--- всё получится эквивалентное.
	\end{conseq}

\subsection[BPP и понижение ошибки]{$\BPP$ и понижение ошибки}
	\begin{Def}
		$L \in \BPP$ (двухсторонняя ограниченная ошибка), если имеется п.о. п.п отношение $R$ такое, что:
		\begin{enumerate}
			\item $x \notin L \Ra \Pr_w \{ (x, w) \in R \} < \frac 13$
			\item $x \in L \Ra \Pr_w \{ (x, w) \in R \} > \frac 23$
		\end{enumerate}
	\end{Def}
	\begin{assertion}
		Неравенство Чернова: если есть независимые случайные величины $x_i \in [0, 1]$ и матожидание их суммы равно $\mu$, то для любого $0<\epsilon<1$:
		\[
			\Pr\{ \sum x_i \ge (1+\epsilon)\mu \} < e^{-\frac{\mu \epsilon^2}{4}}
		\]
	\end{assertion}
	\begin{proof}
		Без доказательства "--- считается за известное.
	\end{proof}
	\begin{theorem}
		В $\BPP$ тоже можно понижать ошибку, т.е. для любого языка $L \in \BPP$ и полинома $p(n)$ есть п.о. п.п. отношение $R$ такое, что:
		\begin{enumerate}
			\item $x \notin L \Ra \Pr_w \{ (x, w) \in R \} < \frac 1{2^{-p(n)}}$
			\item $x \in L \Ra \Pr_w \{ (x, w) \in R \} > 1 - \frac 1{2^{-p(n)}}$
		\end{enumerate}
	\end{theorem}
	\begin{proof}
		Давайте возьмём отношение $R$ из определения $\BPP$ и повторим проверку $p(|x|)$ раз.
		Получим $p(|x|)$ ответов (обозначим это число за $k$).
		Возьмём тот, который чаще всех встречается (будем считать, что $k$ нечётно).
        Это всё ещё полином.

		Теперь оценим вероятность ошибки.
		У нас есть $k$ случайных величин $x_i$ "--- возникла ли ошибка на шаге $i$.
		Каждая величина "--- либо ноль (с вероятностью $\sfrac 23$), либо единица (с вероятностью $\sfrac 13$).
		Матожидание суммы "--- $\sfrac{k}{3}$.
		Весь алгоритм ошибается тогда и только тогда, когда сумма $x_i$ получилась больше $\sfrac{k}{2}$ (т.е. ошиблись больше, чем в половине раундов).
		Пишем неравенство Чернова при $\epsilon=\sfrac12$:
		\begin{gather*}
			\Pr\{ \sum x_i \ge \left(1+ \frac 12\right)\cdot \frac k3 \} < e^{-\frac{\frac{k}{3}\cdot\frac{1}{4}}{4}} \\
			\Pr\{ \sum x_i \ge \frac k2 \} < e^{-\frac{k}{48}} < 2^{-\frac{k}{48}} < 2^{-k}
		\end{gather*}
		Получили, что вероятность ошибки не больше $2^{-k}$, что и требовалось.
	\end{proof}
	\begin{conseq}
		В определении $\BPP$ константы $\frac 13$ и $\frac 23$ взяты с потолка, можно брать любые, равные $x$ и $1-x$, где $x$ "--- константа,
		отделённая от $\frac 12$ и нуля.
	\end{conseq}
	\begin{Rem}
		На самом деле можно даже брать вообще любые две константы, отделённые друг от друга, нуля и единицы, всё равно получится $\BPP$.
		Вроде доказывается тоже через Чернова, надо чуть аккуратнее.
		\TODO
	\end{Rem}

\subsection[PP и вложенность классов]{$\PP$ и вложенность классов}
	\begin{Def}
		$L \in \PP$ (двухсторонняя неограниченная ошибка), если имеется п.о. п.п отношение $R$ такое, что:
		\begin{enumerate}
			\item $x \notin L \Ra \Pr_w \{ (x, w) \in R \} \le \frac 12$
			\item $x \in L \Ra \Pr_w \{ (x, w) \in R \} > \frac 12$
		\end{enumerate}
	\end{Def}
	\begin{Rem}
		Этот класс почти никогда не нужен.
		Отличие от $\BPP$ в том, что ошибки могут подползать к $\frac12$ сколь угодно близко с ростом $|x|$, и тогда их фиг отделишь.
	\end{Rem}

	\begin{theorem}
		\[ \NP \subseteq \PP \]
	\end{theorem}
	\begin{proof}
		Давайте покажем, что $\SAT \in \PP$, после этого любую задачу из $\NP$ можно будет свести к $\SAT$ и она тоже окажется в $\PP$
		(никаких проблем с вероятностью: она у нас берётся по случайным битам, никак не зависит от того, какие преобразования задачи мы делали).

		Пусть есть формула с $n$ переменными.
		Давайте выберем их значения случайно.
		Если формула выполнилась, выводим <<да>>.
		В противном случае выводим <<да>> с вероятностью $\sfrac 12$.
		Покажем, что вероятности какие надо.

		Пусть $x \notin \SAT$, т.е. формула невыполнима.
		Тогда мы скажем <<$x \in \SAT$>> с вероятностью ровно $\sfrac 12$, что и надо.

		Пусть $x \in \SAT$, т.е. есть $k$ выполняющих наборов ($k > 0$).
		Тогда мы в них попадём с вероятностью $\frac{k}{2^n}$ и скажем <<$x \in \SAT$>>, а второй способ сказать <<да>> "--- не попасть в выполняющий набор, но
		угадать со случайным битом.
		Итого вероятность сказать <<да>>:
		\[ \frac{k}{2^n} + \frac{2^n-k}{2^n} \cdot \frac 12 = \frac{2k+2^n-k}{2^{n+1}} = \frac12 + \frac{k}{2^{n+1}} > \frac 12 \]
		Что и требовалось.
	\end{proof}

	\begin{Rem}
		Диаграмма Хассе для вложенности классов получается такая (стрелка от надкласса к подклассу):
		\begin{center}
			\begin{dot2tex}
				digraph G {
					PP -> {NP BPP coNP}
					{NP BPP} -> RP
					{BPP coNP} -> coRP
					{RP coRP} -> ZPP
				}
			\end{dot2tex}
		\end{center}
	\end{Rem}

\subsection[ZPP]{$\ZPP$}
	\begin{Def}
		\[ \ZPP \eqDef \RP \cap \co\RP \]
	\end{Def}
	\begin{theorem}
		Если $L \in \ZPP$, то есть вероятностная ДМТ, которая безошибочно распознаёт $L$ и \textit{матожидание} времени её работы полиномиально.
	\end{theorem}
	\begin{proof}
		По определению $\ZPP$ имеется два полиномиальных вероятностных алгоритма: $A$ (для $\RP$) и $B$ (для $\co\RP$).

		Давайте запустим их обоих, они отработают за полином.
		Если $A$ выдал <<$x \in L$>>, то он точно прав (в эту сторону алгоритмы из $\RP$ не ошибаются).
		Аналогично, если $B$ выдал <<$x \notin L$>>, то гарантировано $x \notin L$.
		А вот что делать в случае, когда $A$ выдал <<$x \notin L$>>, а $B$ выдал <<$x \in L$>> "--- непонятно.
		Какой-то из двух алгоритмов точно ошибся, это произошло с вероятностью не более $\frac 12$.

		Тогда давайте повторим раунд, и так до бесконечности.
		Матожидание времени работы (если считать, что время работы одного раунда равно $p(|x|)$):
		\[ p(|x|) \cdot \left( 1 + \frac 12\left(1 + \frac12\left(\dots \right)\right)\right) = p(|x|) \left(1+\frac12+\frac14+\frac18+\dots\right) = \O(p(|x|)) \]
	\end{proof}
	
	\begin{assertion}
		Неравенство Маркова: если есть неотрицательная случайная величина $x$ с матожиданием $\mu$, то при $a > 0$:
		\[ \Pr\{ x \ge a \} \le \frac{\mu}{a} \]
	\end{assertion}
	\begin{proof}
		Без доказательства.

		Неформально так: предположим обратное, сдвинем у $x$ все значения $<a$ в ноль, а все значения $\ge a$ "--- в $a$.
		Тогда матожидание может лишь уменьшиться (как и правая часть, т.е. неравенство станет строже), а левая часть не изменится.
		Но тогда $\mu = \Pr\{ x \ge a \} \cdot a$, получим как раз равенство, успех.
	\end{proof}

	\begin{theorem}
		Если для языка $L$ существует вероятностная ДМТ $M$, которая безошибочно его распознаёт и матожидание времени её работы полиномиально,
		то $L \in \ZPP$.
	\end{theorem}
	\begin{proof}
		Пусть нам дали $x$ и мы знаем, что матожидание времени работы $M$ равно $p(|x|)$.
		Давайте запустим $M$ в течение $2p(|x|)$ шагов.
		Подставим в неравенство Маркова случайную величину $t$ (время работы $M$) с матожиданием $p(|x|)$ и $a=2p(|x|)$:
		\[
			\Pr \{ t \ge 2p(|x|) \} \le \frac{p(|x|)}{2p(|x|)} = \frac 12
		\]
		Таким образом, получим, что с вероятностью $\frac 12$ мы узнаем точный ответ.

		Теперь строим алгоритм для распознавания $L$ из $\RP$.
		Он будет делать то же, что и выше.
		Узнали точный ответ "--- сказали, если не успели узнать "--- сказали <<$x \notin L$>>, если это и ошибка, то она произойдёт с вероятностью не более $\sfrac 12$.

		Аналогично для алгоритма из $\co\RP$: если не успели узнать точный ответ "--- сказали <<$x \in L$>>, в эту сторону ошибаться дозволяется.
	\end{proof}
